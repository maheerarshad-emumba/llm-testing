{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query   \n",
      "0  What are the embedding dimensions of GTE-Base ...  \\\n",
      "1  How do recursive retrieval techniques help imp...   \n",
      "2  What are the token limits for Longformer and L...   \n",
      "3  Compare the context precision and context simi...   \n",
      "4  Can LlamaIndex handle multilingual datasets, a...   \n",
      "5  What is the population ratio of males and fema...   \n",
      "6  What are the main differences between LangChai...   \n",
      "7  What was the discount percentage for the blend...   \n",
      "8  What is the release year of the BERT language ...   \n",
      "\n",
      "                                            response   \n",
      "0  The embedding dimensions of the GTE-Base model...  \\\n",
      "1  The provided context discusses Recursive Retri...   \n",
      "2  The token limit for the Longformer model is 40...   \n",
      "3  The context precision scores for HYDE and Subq...   \n",
      "4  The provided context does not include informat...   \n",
      "5  The population ratio of males to females in Ge...   \n",
      "6  Based on the provided context, LangChain suppo...   \n",
      "7  The discount percentage for the blender sold i...   \n",
      "8  The provided context does not include informat...   \n",
      "\n",
      "                                        ground truth  \n",
      "0  Embedding dimensions of GTE-Base is 768 and fo...  \n",
      "1  Recursive retrieval techniques help improve mu...  \n",
      "2  The token limit for Longfomer is 4096 tokens a...  \n",
      "3  The HYDE retrieval technique outperforms the S...  \n",
      "4  Both tools primarily focus on integration with...  \n",
      "5  The population ratio of males to females in Ge...  \n",
      "6  LangChain supports multi-query expansion and s...  \n",
      "7  The discount percentage for the blender sold i...  \n",
      "8   This information is not provided in the context.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns_needed = [\"query\", \"response\", \"ground truth\"]\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Emumba/Documents/genie research/llm-testing-main/llm-testing-main/response validation/response-validation_llm_responses.csv\", usecols=columns_needed)\n",
    "\n",
    "# Display the resulting DataFrame with only the required columns\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision scores saved to precision_results.csv\n"
     ]
    }
   ],
   "source": [
    "import falcon_evaluate.evaluate as fe_eval\n",
    "\n",
    "# Initialize the FalconEvaluator with an empty DataFrame just to access the methods\n",
    "evaluator = fe_eval.FalconEvaluator(df=pd.DataFrame())\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row and calculate precision\n",
    "for index, row in df.iterrows():\n",
    "    model_output = row[\"response\"]      # Assuming 'response' column holds model output\n",
    "    reference = row[\"ground truth\"]     # Assuming 'ground truth' column holds reference text\n",
    "    precision = evaluator.calculate_precision(model_output, reference)\n",
    "    \n",
    "    # Append results to list\n",
    "    results.append({\n",
    "        \"query\": row[\"query\"],            # Assuming 'query' column holds query text\n",
    "        \"response\": model_output,\n",
    "        \"ground_truth\": reference,\n",
    "        \"Precision\": precision\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and save as a new CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "import csv\n",
    "# Write directly to CSV\n",
    "with open(\"precision_results.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(results_df.columns)  # Write the header\n",
    "    writer.writerows(results_df.values)  # Write each row of data\n",
    "\n",
    "print(\"Precision scores saved to precision_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module falcon_evaluate.evaluate in falcon_evaluate:\n",
      "\n",
      "NAME\n",
      "    falcon_evaluate.evaluate\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        FalconEvaluator\n",
      "        TextMetricsCalculator\n",
      "    \n",
      "    class FalconEvaluator(builtins.object)\n",
      "     |  FalconEvaluator(df: pandas.core.frame.DataFrame)\n",
      "     |  \n",
      "     |  FalconEvaluator class used to evaluate text data based on different metrics and context relevancy scoring methods.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: pandas.core.frame.DataFrame)\n",
      "     |      Initializes the FalconEvaluator with a dataframe.\n",
      "     |      Parameters:\n",
      "     |      df (pd.DataFrame): A dataframe containing the data to be evaluated.\n",
      "     |  \n",
      "     |  bleu_score(self) -> float\n",
      "     |      Calculate the BLEU score between candidate and reference texts.\n",
      "     |      Returns:\n",
      "     |      float: The BLEU score.\n",
      "     |  \n",
      "     |  calculate_falcon_score(self, model_output, reference, categories_weights, use_relevance)\n",
      "     |      Calculates and aggregates multiple evaluation metrics along with Falcon scores.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |      model_output (str): The output text from the model.\n",
      "     |      reference (str): The reference text to compare against.\n",
      "     |      weights (list): The weights for each score.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |      tuple: A tuple containing dictionaries of individual scores and aggregated Falcon scores.\n",
      "     |  \n",
      "     |  calculate_precision(self, model_output, reference)\n",
      "     |  \n",
      "     |  calculate_recall(self, model_output, reference)\n",
      "     |  \n",
      "     |  calculate_reference_scores(self, model_output, reference)\n",
      "     |  \n",
      "     |  calculate_text_metrics(self, model_output)\n",
      "     |      Calculate text metrics using TextMetricsCalculator.\n",
      "     |      Parameters:\n",
      "     |      model_output (str): The output text from the model.\n",
      "     |      Returns:\n",
      "     |      dict: A dictionary containing the calculated text metrics.\n",
      "     |  \n",
      "     |  cosine_similarity(self) -> float\n",
      "     |      Calculate the cosine similarity between the TF-IDF vectors of the candidate and reference texts.\n",
      "     |      Returns:\n",
      "     |      float: The cosine similarity score.\n",
      "     |  \n",
      "     |  evaluate(self, use_relevance)\n",
      "     |  \n",
      "     |  jaccard_similarity(self) -> float\n",
      "     |      Calculate the Jaccard similarity between candidate and reference texts.\n",
      "     |      Returns:\n",
      "     |      float: The Jaccard similarity score.\n",
      "     |  \n",
      "     |  semantic_similarity(self) -> float\n",
      "     |      Calculate the semantic similarity between candidate and reference texts using BERT embeddings.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |      float: The semantic similarity score.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class TextMetricsCalculator(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  calculate_ari(self, text)\n",
      "     |  \n",
      "     |  calculate_fk_grade_level(self, text)\n",
      "     |  \n",
      "     |  calculate_perplexity(self, text)\n",
      "     |  \n",
      "     |  calculate_toxicity(self, text)\n",
      "     |  \n",
      "     |  compute_metrics(self, row)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "FILE\n",
      "    c:\\users\\emumba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\falcon_evaluate\\evaluate.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import falcon_evaluate.evaluate as fe_eval\n",
    "\n",
    "# List all available functions and classes in evaluate\n",
    "print(help(fe_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall scores saved to recall_results.csv\n"
     ]
    }
   ],
   "source": [
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row and calculate precision\n",
    "for index, row in df.iterrows():\n",
    "    model_output = row[\"response\"]      # Assuming 'response' column holds model output\n",
    "    reference = row[\"ground truth\"]     # Assuming 'ground truth' column holds reference text\n",
    "    recall = evaluator.calculate_recall(model_output, reference)\n",
    "    \n",
    "    # Append results to list\n",
    "    results.append({\n",
    "        \"query\": row[\"query\"],            # Assuming 'query' column holds query text\n",
    "        \"response\": model_output,\n",
    "        \"ground_truth\": reference,\n",
    "        \"Recall\": recall\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and save as a new CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "import csv\n",
    "# Write directly to CSV\n",
    "with open(\"recall_results.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(results_df.columns)  # Write the header\n",
    "    writer.writerows(results_df.values)  # Write each row of data\n",
    "\n",
    "print(\"Recall scores saved to recall_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
