{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from uptrain import EvalLLM, Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV with query, response, context, and ground truth\n",
    "data = pd.read_csv('llm_responses.csv')\n",
    "\n",
    "# Extract the necessary fields from the CSV\n",
    "queries = data['query']\n",
    "responses = data['response']\n",
    "contexts = data['context']\n",
    "ground_truths = data['ground truth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize UpTrain EvalLLM instance\n",
    "eval_llm = EvalLLM(openai_api_key=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate response relevance metric and save the findings\n",
    "metric_name = 'relevance'\n",
    "metric = Evals.RESPONSE_RELEVANCE\n",
    "\n",
    "results = []\n",
    "for query, response in zip(queries, responses):\n",
    "    metrics_result = {\n",
    "        'query': query,\n",
    "        'response': response\n",
    "    }\n",
    "\n",
    "    # Prepare data for evaluation\n",
    "    data_for_eval = [{\n",
    "        \"question\": query,\n",
    "        \"response\": response\n",
    "    }]\n",
    "\n",
    "    # Evaluate the metric\n",
    "    res = eval_llm.evaluate(data=data_for_eval, checks=[metric])\n",
    "    metrics_result[metric_name] = res[0].get('score_response_relevance', 'N/A')\n",
    "    metrics_result[f'{metric_name}_explanation'] = res[0].get('explanation_response_relevance', 'No explanation provided')\n",
    "\n",
    "    results.append(metrics_result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'evaluation_results_{metric_name}.csv', index=False)\n",
    "\n",
    "print(f\"Evaluation for {metric_name} complete. Results saved to 'evaluation_results_{metric_name}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate response consistency metric and save the findings\n",
    "metric_name = 'consistency'\n",
    "metric = Evals.RESPONSE_CONSISTENCY\n",
    "\n",
    "results = []\n",
    "for query, response, context in zip(queries, responses, contexts):\n",
    "    metrics_result = {\n",
    "        'query': query,\n",
    "        'context': context,\n",
    "        'response': response\n",
    "    }\n",
    "\n",
    "    # Prepare data for evaluation\n",
    "    data_for_eval = [{\n",
    "        \"question\": query,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }]\n",
    "\n",
    "    # Evaluate the metric\n",
    "    res = eval_llm.evaluate(data=data_for_eval, checks=[metric])\n",
    "    metrics_result[metric_name] = res[0].get('score_response_consistency', 'N/A')\n",
    "    metrics_result[f'{metric_name}_explanation'] = res[0].get('explanation_response_consistency', 'No explanation provided')\n",
    "\n",
    "    results.append(metrics_result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'evaluation_results_{metric_name}.csv', index=False)\n",
    "\n",
    "print(f\"Evaluation for {metric_name} complete. Results saved to 'evaluation_results_{metric_name}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate factual accuracy metric and save the findings\n",
    "metric_name = 'factual accuracy'\n",
    "metric = Evals.FACTUAL_ACCURACY\n",
    "\n",
    "results = []\n",
    "for query, response, context in zip(queries, responses, contexts):\n",
    "    metrics_result = {\n",
    "        'query': query,\n",
    "        'context': context,\n",
    "        'response': response\n",
    "    }\n",
    "\n",
    "    # Prepare data for evaluation\n",
    "    data_for_eval = [{\n",
    "        \"question\": query,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }]\n",
    "\n",
    "    # Evaluate the metric\n",
    "    res = eval_llm.evaluate(data=data_for_eval, checks=[metric])\n",
    "    metrics_result[metric_name] = res[0].get('score_factual_accuracy', 'N/A')\n",
    "    metrics_result[f'{metric_name}_explanation'] = res[0].get('explanation_factual_accuracy', 'No explanation provided')\n",
    "\n",
    "    results.append(metrics_result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'evaluation_results_{metric_name}.csv', index=False)\n",
    "\n",
    "print(f\"Evaluation for {metric_name} complete. Results saved to 'evaluation_results_{metric_name}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate response conciseness metric and save the findings\n",
    "metric_name = 'response conciseness'\n",
    "metric = Evals.RESPONSE_CONCISENESS\n",
    "\n",
    "results = []\n",
    "for query, response in zip(queries, responses):\n",
    "    metrics_result = {\n",
    "        'query': query,\n",
    "        'response': response\n",
    "    }\n",
    "\n",
    "    # Prepare data for evaluation\n",
    "    data_for_eval = [{\n",
    "        \"question\": query,\n",
    "        \"response\": response\n",
    "    }]\n",
    "\n",
    "    # Evaluate the metric\n",
    "    res = eval_llm.evaluate(data=data_for_eval, checks=[metric])\n",
    "    metrics_result[metric_name] = res[0].get('score_response_conciseness', 'N/A')\n",
    "    metrics_result[f'{metric_name}_explanation'] = res[0].get('explanation_response_conciseness', 'No explanation provided')\n",
    "\n",
    "    results.append(metrics_result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'evaluation_results_{metric_name}.csv', index=False)\n",
    "\n",
    "print(f\"Evaluation for {metric_name} complete. Results saved to 'evaluation_results_{metric_name}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate response matching metric and save the findings\n",
    "from uptrain import ResponseMatching\n",
    "\n",
    "metric_name = 'response matching llm'\n",
    "#metric = Evals.RESPONSE_CONCISENESS\n",
    "\n",
    "results = []\n",
    "for query, response, ground_truth, context in zip(queries, responses, ground_truths, contexts):\n",
    "    metrics_result = {\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth,\n",
    "        'response': response, \n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "    # Prepare data for evaluation\n",
    "    data_for_eval = [{\n",
    "        \"question\": query,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"response\": response,\n",
    "        \"context\": context\n",
    "    }]\n",
    "\n",
    "    # Evaluate the metric\n",
    "    res = eval_llm.evaluate(data=data_for_eval, checks=[ResponseMatching(method='llm')])\n",
    "    print(res)\n",
    "    metrics_result[metric_name] = res[0].get('score_response_match_llm', 'N/A')\n",
    "    #metrics_result[f'{metric_name}_explanation'] = res[0].get('explanation_response_match', 'No explanation provided')\n",
    "\n",
    "    results.append(metrics_result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'evaluation_results_{metric_name}.csv', index=False)\n",
    "\n",
    "print(f\"Evaluation for {metric_name} complete. Results saved to 'evaluation_results_{metric_name}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate response matching metric and save the findings\n",
    "from uptrain import ResponseMatching\n",
    "\n",
    "metric_name = 'response matching rouge'\n",
    "#metric = Evals.RESPONSE_CONCISENESS\n",
    "\n",
    "results = []\n",
    "for query, response, ground_truth, context in zip(queries, responses, ground_truths, contexts):\n",
    "    metrics_result = {\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth,\n",
    "        'response': response, \n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "    # Prepare data for evaluation\n",
    "    data_for_eval = [{\n",
    "        \"question\": query,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"response\": response,\n",
    "        \"context\": context\n",
    "    }]\n",
    "\n",
    "    # Evaluate the metric\n",
    "    res = eval_llm.evaluate(data=data_for_eval, checks=[ResponseMatching(method='rouge')])\n",
    "    print(res)\n",
    "    metrics_result[metric_name] = res[0].get('score_response_match_rouge', 'N/A')\n",
    "    #metrics_result[f'{metric_name}_explanation'] = res[0].get('explanation_response_match', 'No explanation provided')\n",
    "\n",
    "    results.append(metrics_result)\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'evaluation_results_{metric_name}.csv', index=False)\n",
    "\n",
    "print(f\"Evaluation for {metric_name} complete. Results saved to 'evaluation_results_{metric_name}.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
