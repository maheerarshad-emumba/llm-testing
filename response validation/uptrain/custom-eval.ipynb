{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv('C:/Users/Emumba/Documents/genie research/llm-testing-main/llm-testing-main/response validation/response-validation_llm_responses.csv')\n",
    "\n",
    "# Ensure the data contains the necessary columns\n",
    "required_columns = [\"query\", \"ground truth\", \"response\", \"context\"]\n",
    "if not all(column in data.columns for column in required_columns):\n",
    "    raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "# Mapping data columns to prompt variables\n",
    "prompt_var_to_column_mapping = {\n",
    "    \"query\": \"query\",\n",
    "    \"ground_truth\": \"ground truth\",\n",
    "    \"response\": \"response\",\n",
    "    \"context\": \"context\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prompt = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Evaluate the response based on Accuracy and Factuality. Check if the information provided is accurate, matches the ground truth, and does not include any unverified or fabricated details.\n",
    "\n",
    "5: Perfectly accurate; all information directly aligns with the ground truth and context.\n",
    "4: Mostly accurate, with minor inferred but plausible details.\n",
    "3: Partially accurate, some minor factual inaccuracies present.\n",
    "2: Noticeable inaccuracies with some correct elements.\n",
    "1: Largely inaccurate or fabricated information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_prompt = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Evaluate the response based on Completeness. Does it address all aspects of the question comprehensively, covering main points and important details as per the context?\n",
    "\n",
    "5: Fully complete; all main points and details of the question are covered.\n",
    "4: Mostly complete, with minor details missing but sufficient for the main response.\n",
    "3: Moderately complete, covering only parts of the question.\n",
    "2: Partially complete, with several important details missing.\n",
    "1: Very incomplete, missing most key points.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_prompt = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Evaluate the response based on Relevance. Is the information directly related to the question, without unnecessary or unrelated details?\n",
    "\n",
    "5: Highly relevant; every detail is directly related to the question.\n",
    "4: Mostly relevant, with slight digressions.\n",
    "3: Moderately relevant, with some irrelevant information.\n",
    "2: Partially relevant, with considerable unrelated content.\n",
    "1: Mostly irrelevant, with little focus on the question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from uptrain import CustomPromptEval, EvalLLM, Settings\n",
    "\n",
    "# Define choices and scoring for the adjusted prompt\n",
    "criteria = [\"5\", \"4\", \"3\", \"2\", \"1\"]  # Criteria for evaluation\n",
    "criteria_scores = [5, 4, 3, 2, 1]  # Score range for each criterion\n",
    "\n",
    "# Set up evaluation with the adjusted prompt for response validation\n",
    "eval_llm = EvalLLM(settings=Settings(openai_api_key=\"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\", response_format={\"type\": \"json_object\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-04 12:48:40.952\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 9/9 [00:02<00:00,  4.00it/s]\n",
      "c:\\Users\\Emumba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\uptrain\\operators\\language\\llm.py:271: RuntimeWarning: coroutine 'LLMMulticlient.async_fetch_responses' was never awaited\n",
      "  with ThreadPoolExecutor(max_workers=1) as executor:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "\u001b[32m2024-11-04 12:48:47.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n",
      "\u001b[32m2024-11-04 12:48:48.215\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 9/9 [00:01<00:00,  5.00it/s]\n",
      "\u001b[32m2024-11-04 12:48:54.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n",
      "\u001b[32m2024-11-04 12:48:54.988\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 9/9 [00:01<00:00,  4.95it/s]\n",
      "\u001b[32m2024-11-04 12:49:01.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0  What are the embedding dimensions of GTE-Base ...   \n",
      "1  How do recursive retrieval techniques help imp...   \n",
      "2  What are the token limits for Longformer and L...   \n",
      "3  Compare the context precision and context simi...   \n",
      "4  Can LlamaIndex handle multilingual datasets, a...   \n",
      "5  What is the population ratio of males and fema...   \n",
      "6  What are the main differences between LangChai...   \n",
      "7  What was the discount percentage for the blend...   \n",
      "8  What is the release year of the BERT language ...   \n",
      "\n",
      "                                            response  relevance_score  \n",
      "0  The embedding dimensions of the GTE-Base model...              5.0  \n",
      "1  The provided context discusses Recursive Retri...              3.0  \n",
      "2  The token limit for the Longformer model is 40...              5.0  \n",
      "3  The context precision scores for HYDE and Subq...              3.0  \n",
      "4  The provided context does not include informat...              3.0  \n",
      "5  The population ratio of males to females in Ge...              5.0  \n",
      "6  Based on the provided context, LangChain suppo...              4.0  \n",
      "7  The discount percentage for the blender sold i...              5.0  \n",
      "8  The provided context does not include informat...              3.0  \n",
      "Separate CSV files created for accuracy, completeness, and relevance evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Perform individual evaluations for each criterion\n",
    "accuracy_eval = eval_llm.evaluate(\n",
    "    data=data.to_dict(orient=\"records\"),\n",
    "    checks=[CustomPromptEval(\n",
    "        prompt=accuracy_prompt,\n",
    "        choices=criteria,\n",
    "        choice_scores=criteria_scores,\n",
    "        prompt_var_to_column_mapping=prompt_var_to_column_mapping\n",
    "    )]\n",
    ")\n",
    "\n",
    "completeness_eval = eval_llm.evaluate(\n",
    "    data=data.to_dict(orient=\"records\"),\n",
    "    checks=[CustomPromptEval(\n",
    "        prompt=completeness_prompt,\n",
    "        choices=criteria,\n",
    "        choice_scores=criteria_scores,\n",
    "        prompt_var_to_column_mapping=prompt_var_to_column_mapping\n",
    "    )]\n",
    ")\n",
    "\n",
    "relevance_eval = eval_llm.evaluate(\n",
    "    data=data.to_dict(orient=\"records\"),\n",
    "    checks=[CustomPromptEval(\n",
    "        prompt=relevance_prompt,\n",
    "        choices=criteria,\n",
    "        choice_scores=criteria_scores,\n",
    "        prompt_var_to_column_mapping=prompt_var_to_column_mapping\n",
    "    )]\n",
    ")\n",
    "# Convert each result into its own DataFrame\n",
    "accuracy_df = pd.DataFrame([\n",
    "    {\n",
    "        \"query\": result[\"query\"],\n",
    "        \"response\": result[\"response\"],\n",
    "        \"accuracy_score\": result.get(\"score_custom_prompt\", \"No score\")\n",
    "    }\n",
    "    for result in accuracy_eval\n",
    "])\n",
    "completeness_df = pd.DataFrame([\n",
    "    {\n",
    "        \"query\": result[\"query\"],\n",
    "        \"response\": result[\"response\"],\n",
    "        \"completeness_score\": result.get(\"score_custom_prompt\", \"No score\")\n",
    "    }\n",
    "    for result in completeness_eval\n",
    "])\n",
    "relevance_df = pd.DataFrame([\n",
    "    {\n",
    "        \"query\": result[\"query\"],\n",
    "        \"response\": result[\"response\"],\n",
    "        \"relevance_score\": result.get(\"score_custom_prompt\", \"No score\")\n",
    "    }\n",
    "    for result in relevance_eval\n",
    "])\n",
    "print(relevance_df)\n",
    "\n",
    "# Save each criterion evaluation to a separate CSV file\n",
    "accuracy_df.to_csv(\"accuracy_results.csv\", index=False)\n",
    "completeness_df.to_csv(\"completeness_results.csv\", index=False)\n",
    "relevance_df.to_csv(\"relevance_results.csv\", index=False)\n",
    "\n",
    "print(\"Separate CSV files created for accuracy, completeness, and relevance evaluations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
