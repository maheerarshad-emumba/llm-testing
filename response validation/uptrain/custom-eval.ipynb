{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv('C:/Users/Emumba/Documents/genie research/llm-testing-main/llm-testing-main/response validation/response-validation_llm_responses.csv')\n",
    "\n",
    "# Ensure the data contains the necessary columns\n",
    "required_columns = [\"query\", \"ground truth\", \"response\", \"context\"]\n",
    "if not all(column in data.columns for column in required_columns):\n",
    "    raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "\n",
    "# Mapping data columns to prompt variables\n",
    "prompt_var_to_column_mapping = {\n",
    "    \"query\": \"query\",\n",
    "    \"ground_truth\": \"ground truth\",\n",
    "    \"response\": \"response\",\n",
    "    \"context\": \"context\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_prompt = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Evaluate the response based on Accuracy and Factuality. Check if the information provided is accurate, matches the ground truth, and does not include any unverified or fabricated details.\n",
    "\n",
    "5: Perfectly accurate; all information directly aligns with the ground truth and context.\n",
    "4: Mostly accurate, with minor inferred but plausible details.\n",
    "3: Partially accurate, some minor factual inaccuracies present.\n",
    "2: Noticeable inaccuracies with some correct elements.\n",
    "1: Largely inaccurate or fabricated information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_prompt = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Evaluate the response based on Completeness. Does it address all aspects of the question comprehensively, covering main points and important details as per the context?\n",
    "\n",
    "5: Fully complete; all main points and details of the question are covered.\n",
    "4: Mostly complete, with minor details missing but sufficient for the main response.\n",
    "3: Moderately complete, covering only parts of the question.\n",
    "2: Partially complete, with several important details missing.\n",
    "1: Very incomplete, missing most key points.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_prompt = \"\"\"\n",
    "Question: {query}\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Evaluate the response based on Relevance. Is the information directly related to the question, without unnecessary or unrelated details?\n",
    "\n",
    "5: Highly relevant; every detail is directly related to the question.\n",
    "4: Mostly relevant, with slight digressions.\n",
    "3: Moderately relevant, with some irrelevant information.\n",
    "2: Partially relevant, with considerable unrelated content.\n",
    "1: Mostly irrelevant, with little focus on the question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CustomPromptEval in module uptrain.framework.evals:\n",
      "\n",
      "class CustomPromptEval(ParametricEval)\n",
      " |  CustomPromptEval(*, prompt: str, choices: list[str], choice_scores: Union[list[float], list[int]], eval_type: Literal['classify', 'cot_classify'] = 'cot_classify', prompt_var_to_column_mapping: Optional[dict[str, str]] = None) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CustomPromptEval\n",
      " |      ParametricEval\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'choice_scores': typing.Union[list[float], list[int...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'cls': <class 'uptrain.framework.evals.Cus...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = False\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = None\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"CustomPromptEval\", val...\n",
      " |  \n",
      " |  __signature__ = <Signature (*, prompt: str, choices: list[str], ...app...\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_config = {'protected_namespaces': ()}\n",
      " |  \n",
      " |  model_fields = {'choice_scores': FieldInfo(annotation=Union[list[float...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ParametricEval:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |      \n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```py\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self, *, update: 'dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#model_copy\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/serialization/#modelmodel_dump_json\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  model_post_init(self, _BaseModel__context: 'Any') -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |      Hook into generating the model's CoreSchema.\n",
      " |      \n",
      " |      Args:\n",
      " |          source: The class we are generating a schema for.\n",
      " |              This will generally be the same as the `cls` argument if this is a classmethod.\n",
      " |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `pydantic-core` `CoreSchema`.\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'dict[str, Any] | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Usage docs: https://docs.pydantic.dev/2.9/concepts/json/#json-parsing\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CustomPromptEval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emumba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lazy_loader\\__init__.py:202: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from uptrain import CustomPromptEval, EvalLLM, Settings\n",
    "\n",
    "# Define choices and scoring for the adjusted prompt\n",
    "criteria = [\"5\", \"4\", \"3\", \"2\", \"1\"]  # Criteria for evaluation\n",
    "criteria_scores = [5, 4, 3, 2, 1]  # Score range for each criterion\n",
    "\n",
    "# Set up evaluation with the adjusted prompt for response validation\n",
    "eval_llm = EvalLLM(settings=Settings(openai_api_key=\"\", response_format={\"type\": \"json_object\"}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-04 16:36:47.657\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36muptrain.operators.language.llm\u001b[0m:\u001b[36mfetch_responses\u001b[0m:\u001b[36m268\u001b[0m - \u001b[33m\u001b[1mDetected a running event loop, scheduling requests in a separate thread.\u001b[0m\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.74it/s]\n",
      "c:\\Users\\Emumba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\uptrain\\operators\\language\\llm.py:271: RuntimeWarning: coroutine 'LLMMulticlient.async_fetch_responses' was never awaited\n",
      "  with ThreadPoolExecutor(max_workers=1) as executor:\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "\u001b[32m2024-11-04 16:36:56.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36muptrain.framework.evalllm\u001b[0m:\u001b[36mevaluate\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mLocal server not running, start the server to log data and visualize in the dashboard!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'query': 'What are the embedding dimensions of GTE-Base and GTE-Large models?', 'response': 'The embedding dimensions of the GTE-Base model are 768, and those of the GTE-Large model are 1024. This information can be found in the descriptions of these models within the provided context.', 'context': \"Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP/gte-large-en-v1.5 and Alibaba-NLP/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05\", 'ground truth': 'Embedding dimensions of GTE-Base is 768 and for GTE-Large is 1024', 'score_custom_prompt': 5.0}, {'query': 'How do recursive retrieval techniques help improve multi-step problem-solving?', 'response': 'The provided context discusses Recursive Retrieval as a technique that iteratively refines the retrieval process to improve relevance and accuracy. It uses retrieved information to refine the input query or context, making it suitable for complex queries where initial retrievals may need further refinement. However, the context does not explicitly mention how recursive retrieval techniques help improve multi-step problem-solving. Therefore, I cannot provide an answer based on the given information.', 'context': \"both gave us quite good results! Sentence Window Retrieval : Good for extracting specific lines or sections from PR descriptions, comments, or diffs that are relevant to a query Auto Merging Retrieval : Useful for combining information from multiple PRs, comments, and reviews to provide a comprehensive response Recursive Retrieval : Highly suitable for iterative queries where initial results may need further refinement, such as detailed PR review or code analysis They are provided by framework llama index, and can be integrated with any LLM and embedding model For sentence window retrieval, what if we consider base nodes as well? This May increase the performance even more We can not test Recursive as the pipeline does not give us the retrieved context, and evaluation matrix required that to generate a score, but in pdf based testing, there are scores defined for it because the pipeline used partially satisfies its concepts, that's why it is highlighted red as well but still its better than baseline experiment. Following techniques are implemented, there is a general overview, how these techniques work. Technique Description RAG use case Sentence Window Retrieval Segmenting the corpus into overlapping sentence windows and scoring based on query relevance. Scoring each window based on its relevance to the input query. Providing highest scoring windows as context Suitable for use cases where detailed context from specific segments of text is necessary, question answering summarization. Auto Merging Retrieval Extracting relevant passages from multiple sources and combining them into a cohesive context. Automatically combining passages into a single context. Feeding the merged context Ideal for scenarios where information is scattered across different sources, like creating comprehensive reports synthesis from diverse documents. Recursive Retrieval Iteratively refining the retrieval process to improve relevance and accuracy. Using retrieved information to refine the input query or context. Iterative Retrieval: Repeating the retrieval process with refined queries or contexts. Best suited for complex queries where initial retrievals may need further refinement, such as multi-turn dialogue systems iterative problem-solving tasks Pdfs with tabular data Experiment Results DATA : PDF documents, link mentioned on main page Total queries : 50 Answer Similarity Baseline Experiment Sentence Window Auto Merging Recursive Up train 25% 52% 54% 35% Tonic Validate 44% 67% 68% 39% Context Relevance Baseline Experiment Sentence Window Auto Merging Recursive Up train 45% 69% 77% 33% Tonic Validate 40% 74% 73% 55% DATA : Code files from a GitHub repository, link mentioned on main page Total queries : 10 Context similarity Sentence window Auto Merging Recursive Ragas 92% 96% - Tonic Validate 4/5 4/5 - Deep Eval 46% 40% - Up Train 71% 65% - Tru lens 76.25% 76% - Falcon-evaluate 96 100% - Conclusion Every technique is better than the baseline experiment, and auto merging and sentence window both The Sentence Window Retrieval of Technique is Sentence Window Retrieval. The data of Description is Segmenting the corpus into overlapping sentence windows and scoring based on query relevance.Scoring each window based on its relevance to the input query.Providing highest scoring windows as context. The data of RAG use case is Suitable for use cases where detailed context from specific segments of text is necessary,question answering summarization.. The Auto Merging Retrieval of Technique is Auto Merging Retrieval. The data of Description is Extracting relevant passages from multiple sources and combining them into a cohesive context.Automatically combining passages into a single context.Feeding the merged context. The data of RAG use case is Ideal for scenarios where information is scattered across different sources, likecreating comprehensive reports synthesis from diverse documents.. The Recursive Retrieval of Technique is Recursive Retrieval. The data of Description is Iteratively refining the retrieval process to improve relevance and accuracy.Using retrieved information to refine the input query or context.Iterative Retrieval: Repeating the retrieval process with refined queries or contexts.. The data of RAG use case is Best suited for complex queries where initial retrievals may need further refinement, such as multi-turn dialogue systems iterative problem-solving tasksPdfs with tabular data. The Up train of Answer Similarity is Up train. The data of Baseline Experiment is 25%. The data of Sentence Window is 52%. The data of Auto Merging is 54%. The data of Recursive is 35%. The Tonic Validate of Answer Similarity is Tonic Validate. The data of Baseline Experiment is 44%. The data of Sentence Window is 67%. The data of Auto Merging is 68%. The data of Recursive is 39%. The Up train of Context Relevance is Up train. The data of Baseline Experiment is 45%. The data of Sentence Window is 69%. The data of Auto Merging is 77%. The data of Recursive is 33%. The\", 'ground truth': 'Recursive retrieval techniques help improve multi-step problem-solving by iteratively refining the retrieval process to improve relevance and accuracy. In this technique, retrieved information is used to refine the input query or context, allowing the system to adapt to new insights or needs during the problem-solving process. By repeating the retrieval process with refined queries or contexts, recursive retrieval is particularly suited for complex queries where initial retrievals may not provide the full answer and need further refinement, such as in multi-turn dialogue systems or iterative problem-solving tasks.', 'score_custom_prompt': 3.0}, {'query': 'What are the token limits for Longformer and LED 16k models?', 'response': 'The token limit for the Longformer model is 4096 tokens, and for the LED 16k model, it is 16384 tokens. This information is provided in the context under the descriptions for each model.', 'context': \"for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP/gte-large-en-v1.5 and Alibaba-NLP/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for\", 'ground truth': 'The token limit for Longfomer is 4096 tokens and for LED 16K its 16384 tokens', 'score_custom_prompt': 5.0}, {'query': 'Compare the context precision and context similarity scores for the HYDE and Subquery retrieval techniques.', 'response': \"The context precision scores for HYDE and Subquery techniques can be found under the 'Context Precision' section. According to the provided context, the data of Subquery is 22% for Baseline Experiment, 51% for Multiquery, 29% for Subquery, 100% for HYDE, 44% for Multi step, 43% for Step back, and 49% for Query Rewriting. However, the 'Uptrain' and 'Tonic Validate' techniques do not provide context precision data for Subquery and HYDE in the given context.\\n\\nFor context similarity, the data for Subqueries is 82% for HYDE, 91% for Multi step, and 90% for Step back prompting. The context similarity data for other techniques, including the 'Ragas', 'DeepEval', 'Trulens', and 'Falcon-evaluate' techniques, do not directly compare HYDE and Subquery.\\n\\nIn conclusion, the context precision scores for HYDE and Subquery are available for some evaluation techniques, but not for 'Uptrain' and 'Tonic Validate'. The context similarity scores for Subqueries are available for HYDE, Multi step, and Step back prompting, but not directly comparable to Subquery for other evaluation techniques.\", 'context': 'The Uptrain of Answer Similarity is Uptrain. The data of Baseline Experiment is 25%. The data of Multiquery is 28%. The data of Subquery is 15%. The data of HYDE is 48%. The data of Multi step is 21%. The data of Step back is 27%. The data of Query Rewriting is 39%. The Tonic Validate of Answer Similarity is Tonic Validate. The data of Baseline Experiment is 44%. The data of Multiquery is 44%. The data of Subquery is 30%. The data of HYDE is 64%. The data of Multi step is 48%. The data of Step back is 44%. The data of Query Rewriting is 50%. The Uptrain of Context Precision is Uptrain. The data of Baseline Experiment is 45%. The data of Multiquery is 51%. The data of Subquery is 22%. The data of HYDE is 100%. The data of Multistep is 44%. The data of Step back is 43%. The data of Query Rewriting is 49%. The Tonic Validate of Context Precision is Tonic Validate. The data of Baseline Experiment is 40%. The data of Multiquery is 64%. The data of Subquery is 29%. The data of HYDE is 80%. The data of Multistep is 65%. The data of Step back is 63%. The data of Query Rewriting is 53%. The Ragas of Context similarity is Ragas. The data of Multiquery is 31%. The data of Subqueries is 82%. The data of HYDE is 91%. The data of Multi step is 91%. The data of Step back prompting is 90%. The TonicValidate of Context similarity is TonicValidate. The data of Multiquery is 1.5/5. The data of Subqueries is 3.2/5. The data of HYDE is 4.4/5. The data of Multi step is 1.2/5. The data of Step back prompting is 1/5. The DeepEval of Context similarity is DeepEval. The data of Multiquery is 10%. The data of Subqueries is 40%. The data of HYDE is 85%. The data of Multi step is 10%. The data of Step back prompting is 45%. The Uptrain of Context similarity is Uptrain. The data of Multiquery is 14%. The data of Subqueries is 34%. The data of HYDE is 57%. The data of Multi step is 23%. The data of Step back prompting is 21%. The Trulens of Context similarity is Trulens. The data of Multiquery is 41%. The data of Subqueries is 56%. The data of HYDE is 64%. The data of Multi step is 76%. The data of Step back prompting is 76%. The Falcon-evaluate of Context similarity is Falcon-evaluate. The data of Multiquery is 25%. The data of Subqueries is 50%. The data of HYDE is 96%. The data of Multi step is 40%. The data of Step back prompting is 33%. DATA : PDF documents, link mentioned on main page Total queries : 50 Evaluation Tools are Uptrain & Tonic Validate Evaluation Techniques are Baseline Experiment, Multiquery, Subquery, HYDE, Multi step, Step back, Query Rewriting. Metrics for evaluation are Answer Similarity, Context Precision, Context Similarity. Answer Similarity Baseline Experiment Multiquery Subquery HYDE Multi step Step back Query Rewriting Uptrain 25% 28% 15% 48% 21% 27% 39% Tonic Validate 44% 44% 30% 64% 48% 44% 50% Context Precision Baseline Experiment Multiquery Subquery\\n\\nHYDE Multi step Step back Query Rewriting Uptrain 45% 51% 22% 100% 44% 43% 49% Tonic Validate 40% 64% 29% 80% 65% 63% 53% Context similarity Multiquery Subqueries HYDE Multi step Step back prompting Ragas 31% 82% 91% 91% 90% Tonic Validate 1.5/5 3.2/5 4.4/5 1.2/5 1/5 DeepEval 10% 40% 85% 10% 45% Uptrain 14% 34% 57% 23% 21% Trulens 41% 56% 64% 76% 76% Falcon-evaluate 25% 50% 96% 40% 33% DATA : Code files from a GitHub repository, link mentioned on main page Total queries : 10 Conclusion HYDE performs best! - Expands search space, reduces hallucinations, we perform answer to answer embedding vector stores similarity that is resulting in better results. All other techniques, They highly depend on type of query user is going for. These techniques result in a lot of hallucinations. Therefore, on fairly decent queries present in our dataset, their scores lie near our baseline experiment except Hyde and query rewriting.', 'ground truth': 'The HYDE retrieval technique outperforms the Subquery technique in both context precision and context similarity metrics. For context precision, HYDE achieves a perfect score of 100% in the Uptrain evaluation and 80% in Tonic Validate, while Subquery lags significantly with 22% in Uptrain and 29% in Tonic Validate. In terms of context similarity, HYDE again outperforms Subquery across all metrics: it scores 91% in Ragas, 4.4/5 in Tonic Validate, 85% in DeepEval, 57% in Uptrain, 64% in Trulens, and 96% in Falcon-evaluate. In contrast, Subquery scores 82% in Ragas, 3.2/5 in Tonic Validate, 40% in DeepEval, 34% in Uptrain, 56% in Trulens, and 50% in Falcon-evaluate. This comparison clearly shows that HYDE provides better overall performance in both context precision and context similarity.', 'score_custom_prompt': 3.0}, {'query': 'Can LlamaIndex handle multilingual datasets, and how does it compare to LangChain in this regard?', 'response': \"The provided context does not include information about LlamaIndex's capability to handle multilingual datasets. However, it is mentioned that LangChain integrates with multiple LLM providers like OpenAI, Cohere, and Hugging Face Hub, which may include models capable of handling multilingual data. Still, the specifics of LangChain's multilingual handling are not detailed.\\n\\nTo summarize, the information is not available in the provided documents regarding LlamaIndex's capability, and specifics about LangChain's multilingual handling are also not provided.\", 'context': \"The Supported Languages of Analysis between Tools is Supported Languages. The data of LlamaIndex is Python and TypeScript. The data of LangChain is Python and JavaScript. The data of Haystack is Python. The data of Hugging Face is Python. The Main Focus of Analysis between Tools is Main Focus. The data of LlamaIndex is Indexing and efficient retrieval. The data of LangChain is General-purpose LLM applications RAG approachcontext-aware apps. The data of Haystack is Semantic search and Q&A systems. The data of Hugging Face is Pre-trained models, transfer learningwide range of NLP tasks. The Key Features of Analysis between Tools is Key Features. The data of LlamaIndex is Integration with diverse data sources (160+)list indexcustomizable workflows. The data of LangChain is Modular design, flexible, integration, development, deployment, observability, access to various LLMs.. The data of Haystack is Vendor neutralityinteroperabilitycustomizabilityextensibility specialized components. The data of Hugging Face is Pre-trained modelsextensive documentationlarge community easy transfer learningcustomization. The Pros of Analysis between Tools is Pros. The data of LlamaIndex is Handles wide range of data formatsAdvanced indexing techniquesCustomizable RAG process. The data of LangChain is Modular architecture Extensive ecosysteAccess to various LLM providers Ease of use. The data of Haystack is Designed for search and Q&ASpecialized componentsIntegrated workflow. The data of Hugging Face is Wide range of state-of-the-art modelUser-friendly APILarge communityCustomizable. The Cons of Analysis between Tools is Cons. The data of LlamaIndex is Limited to data retrieval, less suitable for complex NLP tasks. The data of LangChain is Generalized, not specialized- Performance may vary based on use case. The data of Haystack is Narrower scope Primarily focused on document understanding and retrieval tasks. The data of Hugging Face is Not specifically focused on search or retrieval Here's a comparative tables based on comparison for different tools like LlamaIndex, LangChain, Haystack, and Hugging Face against different categories like supported languages, main focus, key features, pros, cons, uses cases, vendor integration, Community and Ecosystem, performance optimization, and example libraries/tools. Definition Analysis between Tools Analysis between Tools LlamaIndex LangChain Haystack Hugging Face Supported Languages\\n\\nPython and TypeScript Python and JavaScript Python Python Main Focus\\n\\nIndexing and efficient retrieval General-purpose LLM applications RAG approach context-aware apps Semantic search and Q&A systems Pre-trained models, transfer learning wide range of NLP tasks Key Features\\n\\nIntegration with diverse data sources (160+) list index customizable workflows Modular design, flexible, integration, development, deployment, observability, access to various LLMs. Vendor neutrality interoperability customizability extensibility specialized components Pre-trained models extensive documentation large community easy transfer learning customization Pros\\n\\nHandles wide range of data formats Advanced indexing techniques Customizable RAG process Modular architecture Extensive ecosyste Access to various LLM providers Ease of use Designed for search and Q&A Specialized components Integrated workflow Wide range of state-of-the-art model User-friendly API Large community Customizable Cons\\n\\nLimited to data retrieval, less suitable for complex NLP tasks Generalized, not specialized- Performance may vary based on use case Narrower scope Primarily focused on document understanding and retrieval tasks Not specifically focused on search or retrieval systems Use Cases\\n\\nQ&A chatbots Document understanding and extraction Autonomous agents Prototyping and experimentation Enterprise-level applications Enterprise search and QA Specialized knowledge management Text classification Question answering Language translation Vendor Integration systems. The Use Cases of Analysis between Tools is Use Cases. The data of LlamaIndex is Q&A chatbots Document understanding and extraction Autonomous agents. The data of LangChain is Prototyping and experimentation Enterprise-level applications. The data of Haystack is Enterprise search and QA Specialized knowledge management. The data of Hugging Face is Text classification Question answering Language translation. The Vendor Integration of Analysis between Tools is Vendor Integration. The data of LlamaIndex is Customizable with different retrieval strategies. The data of LangChain is Integrates with multiple LLM providers like OpenAI, Cohere, Hugging Face Hub. The data of Haystack is Integration with various model providers and databases. The data of Hugging Face is Provides a wide range of pre-trained models from different providers. The Community and Ecosystem of Analysis between Tools is Community and Ecosystem. The data of LlamaIndex is Growing community, lightweight interface. The data of LangChain is Large and active community, well-documented examples. The data of Haystack is Well-documented API, growing community. The data of Hugging Face is Extensive community and resources, active user base. The Performance Optimization of Analysis between Tools is Performance Optimization. The data of LlamaIndex is Focused on efficient indexing and retrieval. The data of LangChain is Flexible but may not match performance of tailored solutions for specific tasks. The data of Haystack is Optimized for search and Q&A, better performance in semantic search. The data of Hugging Face is Optimized for various NLP tasks, transfer learning capabilities. The Example Libraries/Tools of Analysis between Tools is Example Libraries/Tools. The data of LlamaIndex is . The data of LangChain is LangSmith, LangServe. The data of Haystack is . The data of Hugging Face is Transformers. The Query Optimisation of Feature is Query Optimisation. The data of LlamaIndex is . The data of\", 'ground truth': \"Both tools primarily focus on integration with various Large Language Models (LLMs) and embedding providers, such as OpenAI, Cohere, and Hugging Face, many of which support multiple languages. LlamaIndex offers customizable retrieval strategies and handles a wide range of data formats, suggesting flexibility in processing different types of data, potentially including multilingual content. LangChain integrates with multiple LLM providers and emphasizes ease of use and access to various LLMs, which could include multilingual models. Therefore, LlamaIndex can handle multilingual datasets to the extent that the underlying LLMs and embeddings it utilizes support multiple languages. Similarly, LangChain's ability to process multilingual data depends on the capabilities of the integrated LLMs. In this regard, both LlamaIndex and LangChain are comparable, as their support for multilingual datasets relies on the language support provided by the LLMs they integrate with rather than inherent multilingual processing features within the tools themselves.\", 'score_custom_prompt': 3.0}, {'query': 'What is the population ratio of males and females in Germany and India?', 'response': 'The population ratio of males to females in Germany is 68% to 32%, and in India, it is 65% to 35%. These ratios are directly provided in the context.', 'context': 'Country Gender Population Ratio Pakistan Male 45% Female 55% India Male 65% Female 35% Germany Male 68% Female 32%\\n\\nThe Pakistan of Country is Pakistan. The data of Gender is Male. The data of Population Ratio is 45%. The Female of Country is Female. The data of Gender is 55%. The India of Country is India. The data of Gender is Male. The data of Population Ratio is 65%. The Female of Country is Female. The data of Gender is 35%. The Germany of Country is Germany. The data of Gender is Male. The data of Population Ratio is 68%. The Female of Country is Female. The data of Gender is 32%. Below is the table that contains information about the cities and the available houses and their prices according to whether they are furnished or not. Location Paris Berlin London Rome Furnished Yes No Yes No Yes No Yes No Studio 350 250 395 300 250 150 360 300 Apartment 700 600 850 650 500 400 750 650 Single story 1250 1000 1550 1300 1000 800 1350 1100 Double story 2250 1800 2500 2000 2000 1750 2400 2250 Villa 5500 4500 6000 4500 4000 3000 5500 4800\\n\\nThe Furnished of Location is Furnished. The data of Paris is Yes. The data of Berlin is No. The data of London is Yes. The data of Rome is No. The Studio of Location is Studio. The data of Paris is 350. The data of Berlin is 250. The data of London is 395. The data of Rome is 300. The Apartment of Location is Apartment. The data of Paris is 700. The data of Berlin is 600. The data of London is 850. The data of Rome is 650. The Single story of Location is Single story. The data of Paris is 1250. The data of Berlin is 1000. The data of London is 1550. The data of Rome is 1300. The Double story of Location is Double story. The data of Paris is 2250. The data of Berlin is 1800. The data of London is 2500. The data of Rome is 2000. The Villa of Location is Villa. The data of Paris is 5500. The data of Berlin is 4500. The data of London is 6000. The data of Rome is 4500. is 41%. The data of Subqueries is 56%. The data of HYDE is 64%. The data of Multi step is 76%. The data of Step back prompting is 76%. The Falcon-evaluate of Context similarity is Falcon-evaluate. The data of Multiquery is 25%. The data of Subqueries is 50%. The data of HYDE is 96%. The data of Multi step is 40%. The data of Step back prompting is 33%.', 'ground truth': 'The population ratio of males to females in Germany is 68% to 32%, and in India, it is 65% to 35%. ', 'score_custom_prompt': 5.0}, {'query': 'What are the main differences between LangChain and Haystack in query optimization strategies?', 'response': 'Based on the provided context, LangChain supports query optimization strategies such as sub-query expansion, Hyde-adapter for embeddings, and rewrite queries. On the other hand, Haystack does not support these query optimization strategies. However, Haystack does support documents optimization through chunking strategies and metadata attachment. It also supports a hierarchical index structure and knowledge graph index, but the context does not mention if LangChain supports these features.', 'context': \"The Supported Languages of Analysis between Tools is Supported Languages. The data of LlamaIndex is Python and TypeScript. The data of LangChain is Python and JavaScript. The data of Haystack is Python. The data of Hugging Face is Python. The Main Focus of Analysis between Tools is Main Focus. The data of LlamaIndex is Indexing and efficient retrieval. The data of LangChain is General-purpose LLM applications RAG approachcontext-aware apps. The data of Haystack is Semantic search and Q&A systems. The data of Hugging Face is Pre-trained models, transfer learningwide range of NLP tasks. The Key Features of Analysis between Tools is Key Features. The data of LlamaIndex is Integration with diverse data sources (160+)list indexcustomizable workflows. The data of LangChain is Modular design, flexible, integration, development, deployment, observability, access to various LLMs.. The data of Haystack is Vendor neutralityinteroperabilitycustomizabilityextensibility specialized components. The data of Hugging Face is Pre-trained modelsextensive documentationlarge community easy transfer learningcustomization. The Pros of Analysis between Tools is Pros. The data of LlamaIndex is Handles wide range of data formatsAdvanced indexing techniquesCustomizable RAG process. The data of LangChain is Modular architecture Extensive ecosysteAccess to various LLM providers Ease of use. The data of Haystack is Designed for search and Q&ASpecialized componentsIntegrated workflow. The data of Hugging Face is Wide range of state-of-the-art modelUser-friendly APILarge communityCustomizable. The Cons of Analysis between Tools is Cons. The data of LlamaIndex is Limited to data retrieval, less suitable for complex NLP tasks. The data of LangChain is Generalized, not specialized- Performance may vary based on use case. The data of Haystack is Narrower scope Primarily focused on document understanding and retrieval tasks. The data of Hugging Face is Not specifically focused on search or retrieval Here's a comparative tables based on comparison for different tools like LlamaIndex, LangChain, Haystack, and Hugging Face against different categories like supported languages, main focus, key features, pros, cons, uses cases, vendor integration, Community and Ecosystem, performance optimization, and example libraries/tools. Definition Analysis between Tools Analysis between Tools LlamaIndex LangChain Haystack Hugging Face Supported Languages\\n\\nPython and TypeScript Python and JavaScript Python Python Main Focus\\n\\nIndexing and efficient retrieval General-purpose LLM applications RAG approach context-aware apps Semantic search and Q&A systems Pre-trained models, transfer learning wide range of NLP tasks Key Features\\n\\nIntegration with diverse data sources (160+) list index customizable workflows Modular design, flexible, integration, development, deployment, observability, access to various LLMs. Vendor neutrality interoperability customizability extensibility specialized components Pre-trained models extensive documentation large community easy transfer learning customization Pros\\n\\nHandles wide range of data formats Advanced indexing techniques Customizable RAG process Modular architecture Extensive ecosyste Access to various LLM providers Ease of use Designed for search and Q&A Specialized components Integrated workflow Wide range of state-of-the-art model User-friendly API Large community Customizable Cons\\n\\nLimited to data retrieval, less suitable for complex NLP tasks Generalized, not specialized- Performance may vary based on use case Narrower scope Primarily focused on document understanding and retrieval tasks Not specifically focused on search or retrieval systems Use Cases\\n\\nQ&A chatbots Document understanding and extraction Autonomous agents Prototyping and experimentation Enterprise-level applications Enterprise search and QA Specialized knowledge management Text classification Question answering Language translation Vendor Integration of LangChain is . The data of Haystack is . The data of Hugging Face is . The a) Multi-query expansion of Feature is a) Multi-query expansion. The data of LlamaIndex is ✅. The data of LangChain is ✅. The data of Haystack is ❌. The data of Hugging Face is ❌. The b) Sub-query expansion of Feature is b) Sub-query expansion. The data of LlamaIndex is ✅. The data of LangChain is ✅. The data of Haystack is ❌. The data of Hugging Face is ❌. The c) Hyde-adapter for embeddings of Feature is c) Hyde-adapter for embeddings. The data of LlamaIndex is ✅. The data of LangChain is ✅. The data of Haystack is ✅. The data of Hugging Face is ❌. The d) Rewrite queries of Feature is d) Rewrite queries. The data of LlamaIndex is ❌. The data of LangChain is ✅. The data of Haystack is ❌. The data of Hugging Face is ✅. The Documents Optimisation of Feature is Documents Optimisation. The data of LlamaIndex is . The data of LangChain is . The data of Haystack is . The data of Hugging Face is . The a) Chunking strategies of Feature is a) Chunking strategies. The data of LlamaIndex is ✅. The data of LangChain is ✅. The data of Haystack is ❌. The data of Hugging Face is ❌. The b) Metadata attachment of Feature is b) Metadata attachment. The data of LlamaIndex is ✅. The data of LangChain is ✅. The data of Haystack is ❌. The data of Hugging Face is ❌. The c) Hierarchical index structure of Feature is c) Hierarchical index structure. The data of LlamaIndex is ✅. The data of LangChain is ❌. The data of Haystack is ❌. The data of Hugging Face is ❌. The d) Knowledge graph index of Feature is d) Knowledge graph index. The data of LlamaIndex is ✅. The data of LangChain is ❌. The data of Haystack is ✅. The data of Hugging Face is ❌. The a) Ollama of Embeddings Model is a) Ollama. The data of is ✅. The data of is ✅. The data of is ✅. The data of is ✅. The b) Cohere of Embeddings Model is b) Cohere. The data of is ✅. The data of is ✅. The data of is ✅. The data of is ✅. The c) Jina of Embeddings Model is\", 'ground truth': 'LangChain supports multi-query expansion and sub-query expansion, enabling it to generate multiple variations of a query and break down complex queries into simpler components, thereby improving retrieval effectiveness and search accuracy. It also supports query rewriting, which allows it to reformulate queries to better align with the underlying data for more relevant results. In contrast, Haystack does not support multi-query expansion, sub-query expansion, or query rewriting. However, both LangChain and Haystack utilize the HyDE-adapter for embeddings to enhance query understanding. Therefore, LangChain provides more extensive query optimization capabilities compared to Haystack, offering additional features that are not available in Haystack.', 'score_custom_prompt': 4.0}, {'query': 'What was the discount percentage for the blender sold in Dublin, according to your sales data?', 'response': 'The discount percentage for the blender sold in Dublin is 60.00%. This information can be found under the data for the Dublin store, which is in Ireland (EMEA region), dated June 18, 2018. The item is a blender, sold by Robert Harris. The list price was 70, the actual price was 28, and the discount percentage was 60.00%.', 'context': 'of List Price is 1000. The data of Actual Price is 360. The data of Discount % is 64.00%. The Dublin of Store is Dublin. The data of Country is Ireland. The data of Region is EMEA. The data of Date is 6/18/2018. The data of Item is Blender. The data of Salesperson is Robert Harris. The data of List Price is 70. The data of Actual Price is 28. The data of Discount % is 60.00%. The Madria of Store is Madria. The data of Country is Spain. The data of Region is EMEA. The data of Date is 6/20/2018. The data of Item is Iron. The data of Salesperson is Roy Nunes. The data of List Price is 150. The data of Actual Price is 65. The data of Discount % is 56.67%. The Kuala Lumpur of Store is Kuala Lumpur. The data of Country is Malaysia. The data of Region is APAC. The data of Date is 6/21/2018. The data of Item is Microwave. The data of Salesperson is Ian Baker. The data of List Price is 700. The data of Actual Price is 273. The data of Discount % is 61.00%. The Sydney of Store is Sydney. The data of Country is Australia. The data of Region is APAC. The data of Date is 6/22/2018. The data of Item is Blender. The data of Salesperson is Richard Manning. The data of List Price is 150. The data of Actual Price is 45. The data of Discount % is 70.00%. The London of Store is London. The data of Country is UK. The data of Region is EMEA. The data of Date is 6/23/2018. The data of Item is Vacuum Cleaner. The data of Salesperson is Claire Brooks. The data of List Price is 50. The data of Actual Price is 20. The data of Discount % is 60.00%. Store Country Region Date Item Salesperson List Price Actual Price Discount % Capetown South Africa EMEA 6/10/2018 Coffee grinder Noel Bull 50 20 60.00% Sydney Australia APAC 6/12/2018 Iron Christine Davies 50 18 64.00% Sydney Australia APAC 6/13/2018 Toaster Susan Luker 150 68 54.67% Riyadh Saudi Arabia EMEA 6/14/2018 Toaster Victoria Sherwin 500 175 65.00% Tel Aviv Israel EMEA 6/15/2018 Toaster David Isaacs 30 14 53.33% Istanbul Turkey EMEA 6/16/2018 Ceiling fan George Sherwin 700 315 55.00% Kuala Lumpur Malaysia APAC 6/17/2018 Washing Machine Valerie Hook 1000 360 64.00% Dublin Ireland EMEA 6/18/2018 Blender Robert Harris 70 28 60.00% Madria Spain EMEA 6/20/2018 Iron Roy Nunes 150 65 56.67% Kuala Lumpur Malaysia APAC 6/21/2018 Microwave Ian Baker 700 273 61.00% Sydney Australia APAC 6/22/2018 Blender Richard Manning 150 45 70.00% London UK EMEA 6/23/2018 Vacuum Cleaner Claire Brooks 50 20 60.00% The Capetown of Store is Capetown. The data of Country is South Africa. The data of Region is EMEA. The data of Date is 6/10/2018. The data of Item is Coffee grinder. The data of Salesperson is Noel Bull. The data of List Price is 50. The data of Actual Price is 20. The data of Discount % is 60.00%. The Sydney of Store is Sydney. The data of Country is Australia. The data of Region is APAC. The data of Date is 6/12/2018. The data of Item is Iron. The data of Salesperson is Christine Davies. The data of List Price is 50. The data of Actual Price is 18. The data of Discount % is 64.00%. The Sydney of Store is Sydney. The data of Country is Australia. The data of Region is APAC. The data of Date is 6/13/2018. The data of Item is Toaster. The data of Salesperson is Susan Luker. The data of List Price is 150. The data of Actual Price is 68. The data of Discount % is 54.67%. The Riyadh of Store is Riyadh. The data of Country is Saudi Arabia. The data of Region is EMEA. The data of Date is 6/14/2018. The data of Item is Toaster. The data of Salesperson is Victoria Sherwin. The data of List Price is 500. The data of Actual Price is 175. The data of Discount % is 65.00%. The Tel Aviv of Store is Tel Aviv. The data of Country is Israel. The data of Region is EMEA. The data of Date is 6/15/2018. The data of Item is Toaster. The data of Salesperson is David Isaacs. The data of List Price is 30. The data of Actual Price is 14. The data of Discount % is 53.33%. The Istanbul of Store is Istanbul. The data of Country is Turkey. The data of Region is EMEA. The data of Date is 6/16/2018. The data of Item is Ceiling fan. The data of Salesperson is George Sherwin. The data of List Price is 700. The data of Actual Price is 315. The data of Discount % is 55.00%. The Kuala Lumpur of Store is Kuala Lumpur. The data of Country is Malaysia. The data of Region is APAC. The data of Date is 6/17/2018. The data of Item is Washing Machine. The data of Salesperson is Valerie Hook. The data of', 'ground truth': 'The discount percentage for the blender sold in Dublin is 60.00%. ', 'score_custom_prompt': 5.0}, {'query': 'What is the release year of the BERT language model, and is there any update mentioned in the document?', 'response': 'The provided context does not include information on the release year of the BERT language model. Therefore, I cannot provide this information. Additionally, there is no update related to the BERT language model mentioned in the document.', 'context': \"for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP/gte-large-en-v1.5 and Alibaba-NLP/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05\", 'ground truth': 'This information is not provided in the context.', 'score_custom_prompt': 3.0}]\n",
      "                                               query  \\\n",
      "0  What are the embedding dimensions of GTE-Base ...   \n",
      "1  How do recursive retrieval techniques help imp...   \n",
      "2  What are the token limits for Longformer and L...   \n",
      "3  Compare the context precision and context simi...   \n",
      "4  Can LlamaIndex handle multilingual datasets, a...   \n",
      "5  What is the population ratio of males and fema...   \n",
      "6  What are the main differences between LangChai...   \n",
      "7  What was the discount percentage for the blend...   \n",
      "8  What is the release year of the BERT language ...   \n",
      "\n",
      "                                            response  relevance_score  \n",
      "0  The embedding dimensions of the GTE-Base model...              5.0  \n",
      "1  The provided context discusses Recursive Retri...              3.0  \n",
      "2  The token limit for the Longformer model is 40...              5.0  \n",
      "3  The context precision scores for HYDE and Subq...              3.0  \n",
      "4  The provided context does not include informat...              3.0  \n",
      "5  The population ratio of males to females in Ge...              5.0  \n",
      "6  Based on the provided context, LangChain suppo...              4.0  \n",
      "7  The discount percentage for the blender sold i...              5.0  \n",
      "8  The provided context does not include informat...              3.0  \n",
      "Separate CSV files created for accuracy, completeness, and relevance evaluations.\n"
     ]
    }
   ],
   "source": [
    "# Perform individual evaluations for each criterion\n",
    "# accuracy_eval = eval_llm.evaluate(\n",
    "#     data=data.to_dict(orient=\"records\"),\n",
    "#     checks=[CustomPromptEval(\n",
    "#         prompt=accuracy_prompt,\n",
    "#         choices=criteria,\n",
    "#         choice_scores=criteria_scores,\n",
    "#         prompt_var_to_column_mapping=prompt_var_to_column_mapping\n",
    "#     )]\n",
    "# )\n",
    "\n",
    "# completeness_eval = eval_llm.evaluate(\n",
    "#     data=data.to_dict(orient=\"records\"),\n",
    "#     checks=[CustomPromptEval(\n",
    "#         prompt=completeness_prompt,\n",
    "#         choices=criteria,\n",
    "#         choice_scores=criteria_scores,\n",
    "#         prompt_var_to_column_mapping=prompt_var_to_column_mapping\n",
    "#     )]\n",
    "# )\n",
    "\n",
    "relevance_eval = eval_llm.evaluate(\n",
    "    data=data.to_dict(orient=\"records\"),\n",
    "    checks=[CustomPromptEval(\n",
    "        prompt=relevance_prompt,\n",
    "        choices=criteria,\n",
    "        choice_scores=criteria_scores,\n",
    "        eval_type='cot_classify',\n",
    "        prompt_var_to_column_mapping=prompt_var_to_column_mapping\n",
    "    )]\n",
    ")\n",
    "print(relevance_eval)\n",
    "# Convert each result into its own DataFrame\n",
    "# accuracy_df = pd.DataFrame([\n",
    "#     {\n",
    "#         \"query\": result[\"query\"],\n",
    "#         \"response\": result[\"response\"],\n",
    "#         \"accuracy_score\": result.get(\"score_custom_prompt\", \"No score\")\n",
    "#     }\n",
    "#     for result in accuracy_eval\n",
    "# ])\n",
    "# completeness_df = pd.DataFrame([\n",
    "#     {\n",
    "#         \"query\": result[\"query\"],\n",
    "#         \"response\": result[\"response\"],\n",
    "#         \"completeness_score\": result.get(\"score_custom_prompt\", \"No score\")\n",
    "#     }\n",
    "#     for result in completeness_eval\n",
    "# ])\n",
    "relevance_df = pd.DataFrame([\n",
    "    {\n",
    "        \"query\": result[\"query\"],\n",
    "        \"response\": result[\"response\"],\n",
    "        \"relevance_score\": result.get(\"score_custom_prompt\", \"No score\")\n",
    "    }\n",
    "    for result in relevance_eval\n",
    "])\n",
    "print(relevance_df)\n",
    "\n",
    "# Save each criterion evaluation to a separate CSV file\n",
    "# accuracy_df.to_csv(\"accuracy_results.csv\", index=False)\n",
    "# completeness_df.to_csv(\"completeness_results.csv\", index=False)\n",
    "relevance_df.to_csv(\"relevance_results2.csv\", index=False)\n",
    "\n",
    "print(\"Separate CSV files created for accuracy, completeness, and relevance evaluations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
