{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a sample dataset with queries, responses, ground truth, and context\n",
    "data = {\n",
    "    \"query\": [\n",
    "        \"What is MLflow?\", \n",
    "        \"Who developed MLflow?\", \n",
    "        \"Explain MLflow's tracking feature.\", \n",
    "        \"What is Databricks?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"MLflow is an open-source platform for managing the ML lifecycle.\",\n",
    "        \"MLflow was developed by Databricks.\",\n",
    "        \"MLflow's tracking feature allows logging and tracking of experiments.\",\n",
    "        \"Databricks is a cloud-based platform for big data and AI.\"\n",
    "    ],\n",
    "    \"context\": [\n",
    "        \"MLflow is used for tracking experiments and managing machine learning lifecycles.\",\n",
    "        \"MLflow was created by the team at Databricks to simplify ML workflows.\",\n",
    "        \"The tracking feature in MLflow allows data scientists to record parameters, metrics, and outputs of experiments.\",\n",
    "        \"Databricks provides a collaborative environment for data scientists and engineers to work on data and machine learning.\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"MLflow is a cloud service for tracking your daily habits and personal tasks.\",  # Hallucinated\n",
    "        \"MLflow was developed by Databricks.\",  # Accurate\n",
    "        \"MLflow's tracking feature allows users to log and track their machine learning experiments.\",  # Accurate\n",
    "        \"Databricks is a fitness app that helps users track workouts and diet.\"  # Hallucinated\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"hallucination_test_data.csv\", index=False)\n",
    "print(\"Dataset created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import make_genai_metric, EvaluationExample\n",
    "\n",
    "# Define sample examples for each score category to help guide the LLM\n",
    "example_accurate = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing the ML lifecycle.\",\n",
    "    score=1,\n",
    "    justification=\"The response is fully accurate based on the ground truth.\"\n",
    ")\n",
    "\n",
    "example_minor_additional = EvaluationExample(\n",
    "    input=\"Who developed MLflow?\",\n",
    "    output=\"MLflow was developed by Databricks, with support from a community of contributors.\",\n",
    "    score=2,\n",
    "    justification=\"The response is accurate but includes additional minor information not explicitly in the ground truth.\"\n",
    ")\n",
    "\n",
    "example_contextual = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow helps with tracking machine learning experiments.\",\n",
    "    score=3,\n",
    "    justification=\"The response is related to MLflow's functionality but does not directly answer the question.\"\n",
    ")\n",
    "\n",
    "example_hallucinated = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is a platform for tracking fitness activities and personal health data.\",\n",
    "    score=4,\n",
    "    justification=\"The response introduces unrelated information that is not in the ground truth or context, making it hallucinated.\"\n",
    ")\n",
    "\n",
    "# Create the custom hallucination metric\n",
    "hallucination_metric = make_genai_metric(\n",
    "    name=\"hallucination_evaluation\",\n",
    "    definition=\"This metric evaluates if a response is accurate, slightly off-topic, or hallucinated based on the provided context and ground truth.\",\n",
    "    grading_prompt=(\n",
    "        \"Hallucination Score:\\n\"\n",
    "        \"- Score 1: Accurate response based on ground truth.\\n\"\n",
    "        \"- Score 2: Mostly accurate with minor additional details.\\n\"\n",
    "        \"- Score 3: Contextually relevant but off-topic.\\n\"\n",
    "        \"- Score 4: Contains fabricated or unrelated information.\"\n",
    "    ),\n",
    "    examples=[example_accurate, example_minor_additional, example_contextual, example_hallucinated],\n",
    "    model=\"openai:/gpt-4\",  # Replace with the OpenAI model endpoint\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\"],\n",
    "    greater_is_better=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/07 10:16:28 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
      "100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 222.50it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 225.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                query  \\\n",
      "0                     What is MLflow?   \n",
      "1               Who developed MLflow?   \n",
      "2  Explain MLflow's tracking feature.   \n",
      "3                 What is Databricks?   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  MLflow is an open-source platform for managing...   \n",
      "1                MLflow was developed by Databricks.   \n",
      "2  MLflow's tracking feature allows logging and t...   \n",
      "3  Databricks is a cloud-based platform for big d...   \n",
      "\n",
      "                                             context  \\\n",
      "0  MLflow is used for tracking experiments and ma...   \n",
      "1  MLflow was created by the team at Databricks t...   \n",
      "2  The tracking feature in MLflow allows data sci...   \n",
      "3  Databricks provides a collaborative environmen...   \n",
      "\n",
      "                                             outputs  \\\n",
      "0  MLflow is a cloud service for tracking your da...   \n",
      "1                MLflow was developed by Databricks.   \n",
      "2  MLflow's tracking feature allows users to log ...   \n",
      "3  Databricks is a fitness app that helps users t...   \n",
      "\n",
      "   hallucination_evaluation/v1/score  \\\n",
      "0                                  4   \n",
      "1                                  1   \n",
      "2                                  1   \n",
      "3                                  4   \n",
      "\n",
      "           hallucination_evaluation/v1/justification  \n",
      "0  The model's response is completely unrelated t...  \n",
      "1  The response accurately answers the question b...  \n",
      "2  The response accurately describes MLflow's tra...  \n",
      "3  The model's response is completely unrelated t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Load the test dataset\n",
    "eval_df = pd.read_csv(\"hallucination_test_data.csv\")\n",
    "\n",
    "# Run evaluation\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.evaluate(\n",
    "        data=eval_df,\n",
    "        evaluators=\"default\",\n",
    "        predictions=\"response\",  # Column with the model's responses\n",
    "        extra_metrics=[hallucination_metric],  # Our custom metric\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"query\",\n",
    "                \"context\": \"context\",\n",
    "                \"ground_truth\": \"ground_truth\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log the hallucination results table as an artifact\n",
    "    results_df = results.tables[\"eval_results_table\"]\n",
    "    results_df.to_csv(\"hallucination_evaluation_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"hallucination_evaluation_results.csv\")\n",
    "\n",
    "# Display results for verification\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex dataset created!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define a complex dataset with diverse response types\n",
    "complex_data = {\n",
    "    \"query\": [\n",
    "        \"What is the main purpose of MLflow?\", \n",
    "        \"Who primarily developed MLflow?\", \n",
    "        \"Describe the tracking feature in MLflow.\", \n",
    "        \"What services does Databricks offer?\", \n",
    "        \"Explain how MLflow integrates with Databricks.\", \n",
    "        \"What is the significance of MLflow's model registry?\",\n",
    "        \"How does MLflow handle data versioning?\", \n",
    "        \"List the components of MLflow.\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"MLflow is primarily designed to manage and streamline the ML lifecycle, including tracking, packaging, and deploying models.\",\n",
    "        \"MLflow was primarily developed by Databricks.\",\n",
    "        \"MLflow's tracking feature enables users to log parameters, metrics, and artifacts from ML experiments.\",\n",
    "        \"Databricks provides a collaborative platform for big data, AI, and ML, with a focus on cloud-based analytics.\",\n",
    "        \"MLflow integrates with Databricks by allowing users to track and log their experiments within the Databricks environment.\",\n",
    "        \"The MLflow model registry enables model versioning, lifecycle management, and deployment.\",\n",
    "        \"MLflow itself does not handle data versioning directly but can be integrated with tools that do.\",\n",
    "        \"MLflow comprises four main components: Tracking, Projects, Models, and Registry.\"\n",
    "    ],\n",
    "    \"context\": [\n",
    "        \"MLflow is an open-source platform that assists with the ML lifecycle by enabling experiment tracking, model packaging, and deployment.\",\n",
    "        \"Databricks developed MLflow as part of its mission to simplify the ML workflow.\",\n",
    "        \"Tracking experiments is a core feature of MLflow, allowing data scientists to log experiment parameters, metrics, and outputs.\",\n",
    "        \"Databricks is known for its cloud-based data and AI platform that enables data scientists to work collaboratively on ML projects.\",\n",
    "        \"MLflow is integrated into Databricks, offering users tools to track and manage their machine learning experiments.\",\n",
    "        \"The MLflow model registry is designed to store and manage different versions of ML models, providing a centralized hub for model management.\",\n",
    "        \"MLflow allows tracking of experiments but relies on other tools for data versioning solutions.\",\n",
    "        \"MLflow includes components for experiment tracking, project management, model packaging, and a model registry for model management.\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"MLflow is a tool that can be used to track machine learning experiments, log model artifacts, and monitor training runs.\",  # Simplistic but correct\n",
    "        \"MLflow was created by Amazon to assist with data management and experiment tracking.\",  # Hallucinated (Amazon)\n",
    "        \"The tracking feature of MLflow helps in tracking fitness activities, personal projects, and daily habits.\",  # Hallucinated\n",
    "        \"Databricks offers cloud-based analytics, data science collaboration, and data storage for healthcare.\",  # Partially correct, adds unrelated detail (healthcare)\n",
    "        \"MLflow integrates with Databricks to allow users to track AI and ML experiments, providing easy deployment options.\",  # Accurate\n",
    "        \"The MLflow model registry is used to store ML models and track metrics across various health sectors.\",  # Partially correct, unrelated domain (health)\n",
    "        \"MLflow directly manages data versioning for all experiment datasets.\",  # Contradictory to ground truth\n",
    "        \"MLflow includes a suite of tools, including Tracking, Projects, Models, and a central Repository for metrics.\"  # Minor detail change, mostly accurate\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "complex_df = pd.DataFrame(complex_data)\n",
    "complex_df.to_csv(\"complex_hallucination_test_data.csv\", index=False)\n",
    "print(\"Complex dataset created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import make_genai_metric, EvaluationExample\n",
    "\n",
    "# Update example responses to guide LLM in complex scoring scenarios\n",
    "example_accurate = EvaluationExample(\n",
    "    input=\"What is the main purpose of MLflow?\",\n",
    "    output=\"MLflow is primarily designed to manage and streamline the ML lifecycle, including tracking, packaging, and deploying models.\",\n",
    "    score=1,\n",
    "    justification=\"The response is fully accurate, directly answering the question based on the ground truth.\"\n",
    ")\n",
    "\n",
    "example_minor_additional = EvaluationExample(\n",
    "    input=\"What is the main purpose of MLflow?\",\n",
    "    output=\"MLflow is a tool for managing the ML lifecycle, including tracking and deploying models, and is widely used by data scientists.\",\n",
    "    score=2,\n",
    "    justification=\"The response is accurate but adds minor additional details ('widely used by data scientists') not in the ground truth.\"\n",
    ")\n",
    "\n",
    "example_contextual = EvaluationExample(\n",
    "    input=\"Describe the tracking feature in MLflow.\",\n",
    "    output=\"MLflow supports machine learning workflows and experiment tracking, especially for data scientists.\",\n",
    "    score=3,\n",
    "    justification=\"The response is contextually relevant but doesn't directly answer the question about MLflow's tracking feature.\"\n",
    ")\n",
    "\n",
    "example_hallucinated = EvaluationExample(\n",
    "    input=\"Who primarily developed MLflow?\",\n",
    "    output=\"MLflow was developed by Amazon as a data management tool.\",\n",
    "    score=4,\n",
    "    justification=\"The response introduces incorrect information about Amazon and data management, making it a hallucination.\"\n",
    ")\n",
    "\n",
    "# Define the hallucination metric with refined examples and descriptions\n",
    "hallucination_metric = make_genai_metric(\n",
    "    name=\"complex_hallucination_evaluation\",\n",
    "    definition=\"This metric evaluates responses based on their accuracy or hallucination. Responses are assessed for correctness, minor additional info, relevance, or hallucination.\",\n",
    "    grading_prompt=(\n",
    "        \"Evaluate the response based on the question, ground truth, and context:\\n\"\n",
    "        \"- Score 1: Accurate response based on ground truth.\\n\"\n",
    "        \"- Score 2: Mostly accurate with minor additional details.\\n\"\n",
    "        \"- Score 3: Contextually relevant but does not directly answer the question.\\n\"\n",
    "        \"- Score 4: Hallucinated or introduces unrelated/fabricated information.\"\n",
    "    ),\n",
    "    examples=[example_accurate, example_minor_additional, example_contextual, example_hallucinated],\n",
    "    model=\"openai:/gpt-4\",  # Specify the model endpoint for LLM evaluation\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\"],\n",
    "    greater_is_better=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/07 10:20:20 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
      "100%|██████████| 8/8 [00:03<00:00,  2.58it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 331.70it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 174.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0                What is the main purpose of MLflow?   \n",
      "1                    Who primarily developed MLflow?   \n",
      "2           Describe the tracking feature in MLflow.   \n",
      "3               What services does Databricks offer?   \n",
      "4     Explain how MLflow integrates with Databricks.   \n",
      "5  What is the significance of MLflow's model reg...   \n",
      "6            How does MLflow handle data versioning?   \n",
      "7                     List the components of MLflow.   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  MLflow is primarily designed to manage and str...   \n",
      "1      MLflow was primarily developed by Databricks.   \n",
      "2  MLflow's tracking feature enables users to log...   \n",
      "3  Databricks provides a collaborative platform f...   \n",
      "4  MLflow integrates with Databricks by allowing ...   \n",
      "5  The MLflow model registry enables model versio...   \n",
      "6  MLflow itself does not handle data versioning ...   \n",
      "7  MLflow comprises four main components: Trackin...   \n",
      "\n",
      "                                             context  \\\n",
      "0  MLflow is an open-source platform that assists...   \n",
      "1  Databricks developed MLflow as part of its mis...   \n",
      "2  Tracking experiments is a core feature of MLfl...   \n",
      "3  Databricks is known for its cloud-based data a...   \n",
      "4  MLflow is integrated into Databricks, offering...   \n",
      "5  The MLflow model registry is designed to store...   \n",
      "6  MLflow allows tracking of experiments but reli...   \n",
      "7  MLflow includes components for experiment trac...   \n",
      "\n",
      "                                             outputs  \\\n",
      "0  MLflow is a tool that can be used to track mac...   \n",
      "1  MLflow was created by Amazon to assist with da...   \n",
      "2  The tracking feature of MLflow helps in tracki...   \n",
      "3  Databricks offers cloud-based analytics, data ...   \n",
      "4  MLflow integrates with Databricks to allow use...   \n",
      "5  The MLflow model registry is used to store ML ...   \n",
      "6  MLflow directly manages data versioning for al...   \n",
      "7  MLflow includes a suite of tools, including Tr...   \n",
      "\n",
      "   complex_hallucination_evaluation/v1/score  \\\n",
      "0                                          1   \n",
      "1                                          4   \n",
      "2                                          4   \n",
      "3                                          2   \n",
      "4                                          1   \n",
      "5                                          4   \n",
      "6                                          4   \n",
      "7                                          1   \n",
      "\n",
      "   complex_hallucination_evaluation/v1/justification  \n",
      "0  The response accurately describes the main pur...  \n",
      "1  The response incorrectly states that Amazon de...  \n",
      "2  The model's response is a hallucination. It in...  \n",
      "3  The response is mostly accurate, providing cor...  \n",
      "4  The response accurately describes how MLflow i...  \n",
      "5  The response introduces unrelated information ...  \n",
      "6  The response is incorrect. MLflow does not dir...  \n",
      "7  The response accurately lists the components o...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Load the complex test dataset\n",
    "eval_df = pd.read_csv(\"complex_hallucination_test_data.csv\")\n",
    "\n",
    "# Run evaluation with the custom hallucination metric\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.evaluate(\n",
    "        data=eval_df,\n",
    "        evaluators=\"default\",\n",
    "        predictions=\"response\",  # Column with the model's responses\n",
    "        extra_metrics=[hallucination_metric],  # Our custom metric\n",
    "        evaluator_config={\n",
    "            \"col_mapping\": {\n",
    "                \"inputs\": \"query\",\n",
    "                \"context\": \"context\",\n",
    "                \"ground_truth\": \"ground_truth\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log the hallucination results table as an artifact\n",
    "    results_df = results.tables[\"eval_results_table\"]\n",
    "    results_df.to_csv(\"complex_hallucination_evaluation_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"complex_hallucination_evaluation_results.csv\")\n",
    "\n",
    "# Display results for verification\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
