{"columns":["name","version","metric_args"],"data":[["hallucination_evaluation","v1",{"name":"hallucination_evaluation","definition":"This metric evaluates if a response is accurate, slightly off-topic, or hallucinated based on the provided context and ground truth.","grading_prompt":"Hallucination Score:\n- Score 1: Accurate response based on ground truth.\n- Score 2: Mostly accurate with minor additional details.\n- Score 3: Contextually relevant but off-topic.\n- Score 4: Contains fabricated or unrelated information.","examples":[{"grading_context":null,"input":"What is MLflow?","justification":"The response is fully accurate based on the ground truth.","output":"MLflow is an open-source platform for managing the ML lifecycle.","score":1},{"grading_context":null,"input":"Who developed MLflow?","justification":"The response is accurate but includes additional minor information not explicitly in the ground truth.","output":"MLflow was developed by Databricks, with support from a community of contributors.","score":2},{"grading_context":null,"input":"What is MLflow?","justification":"The response is related to MLflow's functionality but does not directly answer the question.","output":"MLflow helps with tracking machine learning experiments.","score":3},{"grading_context":null,"input":"What is MLflow?","justification":"The response introduces unrelated information that is not in the ground truth or context, making it hallucinated.","output":"MLflow is a platform for tracking fitness activities and personal health data.","score":4}],"version":"v1","model":"openai:\/gpt-4","grading_context_columns":null,"include_input":true,"parameters":{"temperature":0.0},"aggregations":["mean"],"greater_is_better":false,"max_workers":10,"metric_metadata":null,"mlflow_version":"2.17.2","fn_name":"make_genai_metric"}]]}