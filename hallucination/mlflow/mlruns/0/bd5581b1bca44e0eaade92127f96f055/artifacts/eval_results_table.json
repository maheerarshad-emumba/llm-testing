{"columns":["query","context","ground truth","outputs","hallucination_evaluation\/v1\/score","hallucination_evaluation\/v1\/justification"],"data":[["Does GTE-Large support up to 8192 tokens in a single input like BGE-M3?","Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP\/gte-large-en-v1.5 and Alibaba-NLP\/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05","No, GTE Large supports upto 512 tokens","No, GTE-Large does not support up to 8192 tokens in a single input. It is limited to 512 tokens. This information can be found in the context under the description for GTE-Large.",null,"Failed to extract score and justification. Raw output: score: 1.0\njustification: The response accurately answers the question based on the provided ground truth and context, with no additional or incorrect information."],["Is BGE-M3 primarily used for summarization tasks?","Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for both gave us quite good results! Sentence Window Retrieval : Good for extracting specific lines or sections from PR descriptions, comments, or diffs that are relevant to a query Auto Merging Retrieval : Useful for combining information from multiple PRs, comments, and reviews to provide a comprehensive response Recursive Retrieval : Highly suitable for iterative queries where initial results may need further refinement, such as detailed PR review or code analysis They are provided by framework llama index, and can be integrated with any LLM and embedding model For sentence window retrieval, what if we consider base nodes as well? This May increase the performance even more We can not test Recursive as the pipeline does not give us the retrieved context, and evaluation matrix required that to generate a score, but in pdf based testing, there are scores defined for it because the pipeline used partially satisfies its concepts, that's why it is highlighted red as well but still its better than baseline experiment. It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05","No such information present in context","The provided context does not mention a model named \"BGE-M3\". Therefore, I cannot provide information on whether it is primarily used for summarization tasks.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model correctly states that it cannot provide information on \"BGE-M3\" as it is not mentioned in the provided context. This falls under the 'Information Not Found' category."],["Does GTE-Qwen2-7B-instruct provide specialized support for legal document summarization?","It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for The Sentence Window Retrieval of Technique is Sentence Window Retrieval. The data of Description is Segmenting the corpus into overlapping sentence windows and scoring based on query relevance.Scoring each window based on its relevance to the input query.Providing highest scoring windows as context. The data of RAG use case is Suitable for use cases where detailed context from specific segments of text is necessary,question answering summarization.. The Auto Merging Retrieval of Technique is Auto Merging Retrieval. The data of Description is Extracting relevant passages from multiple sources and combining them into a cohesive context.Automatically combining passages into a single context.Feeding the merged context. The data of RAG use case is Ideal for scenarios where information is scattered across different sources, likecreating comprehensive reports synthesis from diverse documents.. The Recursive Retrieval of Technique is Recursive Retrieval. The data of Description is Iteratively refining the retrieval process to improve relevance and accuracy.Using retrieved information to refine the input query or context.Iterative Retrieval: Repeating the retrieval process with refined queries or contexts.. The data of RAG use case is Best suited for complex queries where initial retrievals may need further refinement, such as multi-turn dialogue systems iterative problem-solving tasksPdfs with tabular data. The Up train of Answer Similarity is Up train. The data of Baseline Experiment is 25%. The data of Sentence Window is 52%. The data of Auto Merging is 54%. The data of Recursive is 35%. The Tonic Validate of Answer Similarity is Tonic Validate. The data of Baseline Experiment is 44%. The data of Sentence Window is 67%. The data of Auto Merging is 68%. The data of Recursive is 39%. The Up train of Context Relevance is Up train. The data of Baseline Experiment is 45%. The data of Sentence Window is 69%. The data of Auto Merging is 77%. The data of Recursive is 33%. The","No such information present in context","The provided context does not mention anything specific about GTE-Qwen2-7B-instruct having specialized support for legal document summarization. It is a multilingual embedding model that supports more than 100 working languages and can process inputs of different granularities. It ranks first in both English and Chinese evaluations on the Massive Text Embedding Benchmark. However, the context does not provide information about its applicability to legal document summarization.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model correctly states that the provided context does not mention anything specific about GTE-Qwen2-7B-instruct having specialized support for legal document summarization. However, it does not provide any additional information or answer the question directly."],["Which model between GTE-Base and GTE-Large performs better for medical research document processing?","Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP\/gte-large-en-v1.5 and Alibaba-NLP\/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It","No such information present in context","The provided context does not include specific information about the performance of GTE-Base and GTE-Large on medical research document processing. Therefore, I cannot provide an accurate comparison between these two models for this specific use case.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model correctly identifies that the provided context does not include specific information about the performance of GTE-Base and GTE-Large on medical research document processing. Therefore, it cannot provide an accurate comparison. This falls under the 'Information Not Found' category."],["Does Recursive Retrieval allow token inputs over 32k?","both gave us quite good results! Sentence Window Retrieval : Good for extracting specific lines or sections from PR descriptions, comments, or diffs that are relevant to a query Auto Merging Retrieval : Useful for combining information from multiple PRs, comments, and reviews to provide a comprehensive response Recursive Retrieval : Highly suitable for iterative queries where initial results may need further refinement, such as detailed PR review or code analysis They are provided by framework llama index, and can be integrated with any LLM and embedding model For sentence window retrieval, what if we consider base nodes as well? This May increase the performance even more We can not test Recursive as the pipeline does not give us the retrieved context, and evaluation matrix required that to generate a score, but in pdf based testing, there are scores defined for it because the pipeline used partially satisfies its concepts, that's why it is highlighted red as well but still its better than baseline experiment. It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP\/gte-large-en-v1.5 and Alibaba-NLP\/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It","No such information present in context","The provided context does not specify whether Recursive Retrieval allows token inputs over 32k. The maximum input tokens for GTE-Qwen2-7b-instruct, which is the model mentioned in the context, is 32k. However, there is no mention of Recursive Retrieval's specific capabilities in this regard.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model's response indicates that the information about Recursive Retrieval's token input capabilities is not found in the provided context. It does not introduce any fabricated or unverified information."],["Is there any embedding model specifically designed for handling tabular data?","for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP\/gte-large-en-v1.5 and Alibaba-NLP\/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 is c) Jina. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is \u2705. The d) Voyage of Embeddings Model is d) Voyage. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is \u2705. The e) OpenAI of Embeddings Model is e) OpenAI. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is \u2705. The f) LLM Embedder of Embeddings Model is f) LLM Embedder. The data of is \u274c. The data of is \u274c. The data of is \u274c. The data of is \u2705. The g) BGE Embeddings of Embeddings Model is g) BGE Embeddings. The data of is \u274c. The data of is \u274c. The data of is \u274c. The data of is \u2705. The a) Sentence window retrieval of Context Expansion is a) Sentence window retrieval. The data of is \u2705. The data of is \u274c. The data of is \u274c. The data of is \u274c. The b) Recursive retrieval of Context Expansion is b) Recursive retrieval. The data of is \u2705. The data of is \u274c. The data of is \u274c. The data of is \u274c. The c) Auto merging of Context Expansion is c) Auto merging. The data of is \u2705. The data of is \u274c. The data of is \u274c. The data of is \u274c. The Context Rerankers of Context Expansion is Context Rerankers. The data of is . The data of is . The data of is . The data of is . The a) Cohere reranker of Context Expansion is a) Cohere reranker. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is \u274c. The b) BGE reranker of Context Expansion is b) BGE reranker. The data of is \u274c. The data of is \u274c. The data of is \u2705. The data of is \u2705. The c) Rank fusion of Context Expansion is c) Rank fusion. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is . The d) ColBERT reranker of Context Expansion is d) ColBERT reranker. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is \u2705. The e) Long context reorder of Context Expansion is e) Long context reorder. The data of is \u2705. The data of is \u2705. The data of is \u274c. The data of is \u274c. The f) Jina reranker of Context Expansion is f) Jina reranker. The data of is \u2705. The data of is \u2705. The data of is \u2705. The data of is \u2705. The g)","No such information present in context","The provided context does not mention any embedding model specifically designed for handling tabular data.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model's response indicates that the information is not found in the context, which aligns with the 'Information Not Found' category in the rubric."],["Does the GTE-Qwen2-7B-instruct model have real-time processing capabilities for large datasets?","It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for systems. The Use Cases of Analysis between Tools is Use Cases. The data of LlamaIndex is Q&A chatbots Document understanding and extraction Autonomous agents. The data of LangChain is Prototyping and experimentation Enterprise-level applications. The data of Haystack is Enterprise search and QA Specialized knowledge management. The data of Hugging Face is Text classification Question answering Language translation. The Vendor Integration of Analysis between Tools is Vendor Integration. The data of LlamaIndex is Customizable with different retrieval strategies. The data of LangChain is Integrates with multiple LLM providers like OpenAI, Cohere, Hugging Face Hub. The data of Haystack is Integration with various model providers and databases. The data of Hugging Face is Provides a wide range of pre-trained models from different providers. The Community and Ecosystem of Analysis between Tools is Community and Ecosystem. The data of LlamaIndex is Growing community, lightweight interface. The data of LangChain is Large and active community, well-documented examples. The data of Haystack is Well-documented API, growing community. The data of Hugging Face is Extensive community and resources, active user base. The Performance Optimization of Analysis between Tools is Performance Optimization. The data of LlamaIndex is Focused on efficient indexing and retrieval. The data of LangChain is Flexible but may not match performance of tailored solutions for specific tasks. The data of Haystack is Optimized for search and Q&A, better performance in semantic search. The data of Hugging Face is Optimized for various NLP tasks, transfer learning capabilities. The Example Libraries\/Tools of Analysis between Tools is Example Libraries\/Tools. The data of LlamaIndex is . The data of LangChain is LangSmith, LangServe. The data of Haystack is . The data of Hugging Face is Transformers. The Query Optimisation of Feature is Query Optimisation. The data of LlamaIndex is . The data of","No such information present in context","The context does not provide specific information about the real-time processing capabilities of the GTE-Qwen2-7B-instruct model for large datasets.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model's response indicates that the information asked in the question is not found in the provided context. It does not introduce any fabricated or unverified information."],["Can Recursive Retrieval merge results from multiple domains like medicine and law?","both gave us quite good results! Sentence Window Retrieval : Good for extracting specific lines or sections from PR descriptions, comments, or diffs that are relevant to a query Auto Merging Retrieval : Useful for combining information from multiple PRs, comments, and reviews to provide a comprehensive response Recursive Retrieval : Highly suitable for iterative queries where initial results may need further refinement, such as detailed PR review or code analysis They are provided by framework llama index, and can be integrated with any LLM and embedding model For sentence window retrieval, what if we consider base nodes as well? This May increase the performance even more We can not test Recursive as the pipeline does not give us the retrieved context, and evaluation matrix required that to generate a score, but in pdf based testing, there are scores defined for it because the pipeline used partially satisfies its concepts, that's why it is highlighted red as well but still its better than baseline experiment. Customizable with different retrieval strategies Integrates with multiple LLM providers like OpenAI, Cohere, Hugging Face Hub Integration with various model providers and databases Provides a wide range of pre-trained models from different providers Community and Ecosystem\r\n\r\nGrowing community, lightweight interface Large and active community, well-documented examples Well-documented API, growing community Extensive community and resources, active user base Performance Optimization\r\n\r\nFocused on efficient indexing and retrieval Flexible but may not match performance of tailored solutions for specific tasks Optimized for search and Q&A, better performance in semantic search Optimized for various NLP tasks, transfer learning capabilities Example Libraries\/Tools\r\n\r\nLangSmith, LangServe Transformers Advance Rag Use-Cases comparison Feature LlamaIndex LangChain Haystack Hugging Face Query Optimisation\r\n\r\na) Multi-query expansion \u2705 \u2705 \u274c \u274c b) Sub-query expansion \u2705 \u2705 \u274c \u274c c) Hyde-adapter for embeddings \u2705 \u2705 \u2705 \u274c d) Rewrite queries \u274c \u2705 \u274c \u2705 Documents Optimisation\r\n\r\na) Chunking strategies \u2705 \u2705 \u274c \u274c b) Metadata attachment \u2705 \u2705 \u274c \u274c c) Hierarchical index structure \u2705 \u274c \u274c \u274c d) Knowledge graph index \u2705 \u274c \u2705 \u274c Embeddings Model\r\n\r\na) Ollama \u2705 \u2705 \u2705 \u2705 b) Cohere \u2705 \u2705 \u2705 \u2705 c) Jina \u2705 \u2705 \u2705 \u2705 d) Voyage \u2705 \u2705 \u2705 \u2705 e) OpenAI \u2705 \u2705 \u2705 \u2705 f) LLM Embedder \u274c \u274c \u274c \u2705 g) BGE Embeddings \u274c \u274c \u274c \u2705 Context Expansion\r\n\r\na) Sentence window retrieval \u2705 \u274c \u274c \u274c b) Recursive retrieval \u2705 \u274c \u274c \u274c c) Auto merging \u2705 \u274c \u274c \u274c Context Rerankers\r\n\r\na) Cohere reranker \u2705 \u2705 \u2705 \u274c b) BGE reranker \u274c \u274c \u2705 \u2705 c) Rank fusion \u2705 \u2705 \u2705 The Sentence Window Retrieval of Technique is Sentence Window Retrieval. The data of Description is Segmenting the corpus into overlapping sentence windows and scoring based on query relevance.Scoring each window based on its relevance to the input query.Providing highest scoring windows as context. The data of RAG use case is Suitable for use cases where detailed context from specific segments of text is necessary,question answering summarization.. The Auto Merging Retrieval of Technique is Auto Merging Retrieval. The data of Description is Extracting relevant passages from multiple sources and combining them into a cohesive context.Automatically combining passages into a single context.Feeding the merged context. The data of RAG use case is Ideal for scenarios where information is scattered across different sources, likecreating comprehensive reports synthesis from diverse documents.. The Recursive Retrieval of Technique is Recursive Retrieval. The data of Description is Iteratively refining the retrieval process to improve relevance and accuracy.Using retrieved information to refine the input query or context.Iterative Retrieval: Repeating the retrieval process with refined queries or contexts.. The data of RAG use case is Best suited for complex queries where initial retrievals may need further refinement, such as multi-turn dialogue systems iterative problem-solving tasksPdfs with tabular data. The Up train of Answer Similarity is Up train. The data of Baseline Experiment is 25%. The data of Sentence Window is 52%. The data of Auto Merging is 54%. The data of Recursive is 35%. The Tonic Validate of Answer Similarity is Tonic Validate. The data of Baseline Experiment is 44%. The data of Sentence Window is 67%. The data of Auto Merging is 68%. The data of Recursive is 39%. The Up train of Context Relevance is Up train. The data of Baseline Experiment is 45%. The data of Sentence Window is 69%. The data of Auto Merging is 77%. The data of Recursive is 33%. The","No such information present in context","The context does not provide information on whether Recursive Retrieval can merge results from multiple domains like medicine and law. It only mentions that Recursive Retrieval is useful for iterative queries where initial results may need further refinement, and it can be integrated with any Language Model and embedding model.",null,"Failed to extract score and justification. Raw output: score: 0.6\njustification: The model's response indicates that the information asked in the question is not found in the provided context. It does not introduce any fabricated or unverified information."],["Does BGE-M3 outperform GTE-Qwen2-7B-instruct on multi-lingual benchmarks?","It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 BGE-M3 outperformed the open AI models in terms of almost everything. Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for","It isn't specifically stated if BGE-M3 outperformed GTE-QWEN2-7B on multilingual benchmarks","The context does not provide a direct comparison between BGE-M3 and GTE-Qwen2-7B-instruct on multilingual benchmarks. However, it mentions that BGE-M3 outperformed open AI models in terms of almost everything, and GTE-Qwen2-7B-instruct is a model from the same category (open-source embedding models). Therefore, it can be inferred that BGE-M3 might outperform GTE-Qwen2-7B-instruct on multilingual benchmarks, but there is no explicit statement in the context to confirm this.",null,"Failed to extract score and justification. Raw output: score: 0.0\njustification: The response introduces fabricated information about BGE-M3 outperforming open AI models, which is not present in the input context. Therefore, it is marked as 'Hallucination.'"],["Are the Longformer Base 4096 and GTE-Large models frequently used together in multi-step pipelines?","Open Source Embedding Models GTE-Base General model for similarity search or downstream enrichments. Used for general text blobs. Limited to 512 tokens Embedding Dimensions: 768 Model size: 219 MB GTE-Large High quality general model for similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 1024 Model Size: 670 MB GTE-Small Good quality general model for faster similarity search or downstream Used for general text blobs Limited to 512 tokens Embedding Dimensions: 384 Model Size: 67 MB E5-Small A good small and fast general model for similarity search or downstream enrichments Used for general text blobs Limited to 512 tokens Embedding Dimension: 384 Model size: 128 MB Multilingual A general model to handle multilingual datasets Used for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size: 1.04 GB RoBERTa A RoBERTa model train on data up to december 2022, for users who are familiar with BERT model family and want to use it in Graft Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 476 MB MPNet V2 Mpnet model with Siamese architecture trained for text similarity Used for similarity search for text Limited to 512 tokens Embedding Dimensions: 768 Model Size: 420 MB Scibert Science-Vocabulary Uncased A BERT model pretrained on scientific text with specific science focused vocabulary Used for scientific text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 442 MB Longformer Base 4096 A transformer model for long text, based on RoBERTa Used for text up to 4096 tokens Limited to 4096 tokens Embedding Dimensions: 768 Model Size: 597 MB Distilbert Base Uncased Relatively fast and small model with near performance to BERT Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 268 MB Bert Base Uncased The BERT Language model trained on English text via masked language modeling and next sentence prediction Used for It can simultaneously perform the three common retrieval functionalities of embedding model: Dense retrieval, multi-vector retrieval, and sparse retrieval. Multi-Linguality: It can support more than 100 working languages Multi-granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Model Size: 2.2 GB GTE-Qwen2-7B-instruct It ranks no.1 in both English and Chinese evaluations on the Massive text embedding benchmark. GTE-Qwen2-7b-instruct is trained on model based on the Qwen2-7b LLM Model. The model incorporates several advancements Integration of bidirectional attention mechanisms, enriching its contextual understanding. Instruction tuning, applied solely on the query side for streamlined efficiency Comprehensive training across a vast, multilingual text corpus spanning diverse domains and scenarios. Model Size: 7B Embedding Dimension: 3584 Max Input Tokens: 32k Best Strategies to Select an Embedding Model MTEB Leaderboard on Hugging Face is a good starting point to shortlist the top notch models. However, it is still not a 100 percent accurate comparison as these results may vary with your dataset. A model with higher NDCG is better at ranking relevant items higher in the list of retrieved results. Model size has a direct impact on latency. The latency performance trade off becomes especially important in a production setup. Max tokens supported by a model indicate the number of tokens that can be compressed into a single embedding. Generally a model with 512 tokens is more than enough Smaller embeddings offer faster inference and are more storage efficient, while higher dimensions can capture nuanced details and relationships in the data. So it will be a trade-off between capturing the complexity of the data and operational efficiency. According to a research presented in this blog: https:\/\/towardsdatascience.com\/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05 for general text blobs Limited to 512 tokens Embedding Dimension: 768 Model Size 440 MB Multilingual BERT A multilingual version of BERT trained on 102 languages Used for scenarios where text of various languages exist Limited to 512 tokens Embedding Dimensions: 768 Model Size: 714 MB E5-Base A good general model for similarity search and downstream enrichment. Used for general text blobs Limited to 512 tokens Embedding Dimensions: 768 Model Size: 418 MB LED 16k A transformer model for very long text, based on BART Used for text up to 16384 tokens Limitation: Compressing 16kish words into 768 dimensions will surely be lossy Embedding Dimensions: 768 Model Size: 648 MB Voyage-Lite-02-Instruct Instruction tuned model from first generation of the Voyage family Used for instruction tuned for classification, clustering and sentence textual similarity tasks, which are the only recommended use cases. Limited to small text embedding model from second generation of Voyage family Embedding Dimension: 1024 Model Size: 1220 MB stella_en_1.5B_v5 Models is trained on Alibaba-NLP\/gte-large-en-v1.5 and Alibaba-NLP\/gte-Qwen2-1.5B-instruct Model is finally trained by MRL and they have multiple dimensions: 512, 768, 1024, 2048, 4086, 6144 and 8192. The higher the dimension, the better the performance. Max tokens: 121072 Model Size (Million Parameters) 1543 SFR-Embedding-2_R Embedding Dimension: 4096 Max Tokens: 32768 Model Size (Million Parameters): 7111 BGE-Large-en-v1.5 Supports more languages, longer texts and other retrieval methods. Max tokens: 512 First embedding model that supports all 3 retrieval methods. Embedding Dimensions: 1024 Model Size (Million Parameters): 335 Jina-Embeddings-V2 Embedding Dimensions: 1536 Max Token: 8191 Model Size: 0.27 GB Best used for legal document analysis, medical research, literary analysis, financial forecasting, conversational AI. BGE-M3 It's distinguished for its versatility in multi-functionality, multi-linguality and multi-granularity. It","No such information present in context","The provided context does not mention or imply the frequent use of the Longformer Base 4096 and GTE-Large models together in multi-step pipelines.",null,"Failed to extract score and justification. Raw output: score: 1.0\njustification: The response accurately answers the question based on the provided context, without adding any additional or incorrect information."]]}